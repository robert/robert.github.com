---
layout: post
title: HTTP for advanced beginners
published: false
---
Book - more practical examples at end of each chapter. Have Kate say them
Http make my own cliffs notes at the end

TODO - maybe move the thing about HTTP only being responsible for message structure up

Http is stateless
Brief dns




You’ve started yet another company with your good friend, Steve Steveington. It’s an online marketplace where people can buy and sell things and no one asks too many questions. It’s basically a rip-off of Craigslist, but with Steve’s name instead of Craig’s.

You're responsible for building the entire Steveslist platform. However, while you do know how to build small web apps, you have no idea how to build a system capable of scaling to millions of users before your investors notice all the accounting fraud that CEO Steve Steveington will inevitably commit behind your back. You've therefore enlisted your less-good-but-still-mostly-reliable friend, Kate Kateberry, to help fill in some gaps in your knowledge. She's already given you an excellent primer on systems design[LINK], the transcript of which picked up a cool 945 points on Hacker News[LINK]. You don't like to boast, but since internet points can't buy sandwiches or make your alimony payments or fill the yawning void inside of you where your self-worth used to be, boasting seems to be the only use left for those 945 Hacker News points. You think that the marketers call this "social proof".

You found Kate's introduction to systems design very insightful, well-structured, and empathetic. Now you hope that she'll help you continue your journey into the details of software engineering. The next thing you want to understand is the HTTP protocol. You know that web browsers and other applications use HTTP to send and receive data to and from servers. But how? Where precisely does HTTP fit in? And what *is* HTTP really? Is it a program? Is it a library? How does it relate to all the other technologies and acronyms that you've heard are so important for the smooth running of the internet?

As the Steveslist platform has become more complex, you've started to want and need to care about these kinds of details. You want to design your systems to make full use of the different features of HTTP. You want to use and understand HTTP-based APIs. And you want to use HTTP as an accessible entry point into other subjects that you'd like to learn about, such as lower-level computer networking and how web browsers work.

You especially want to know:

* What HTTP is and how it works
* How HTTP is used in the real world
* The most important features of HTTP
* Why these features exist and how they relate to the real world
* How to work with HTTP in your own programs
* How HTTPS keeps HTTP messages secure
* Case studies of interesting uses and side-effects of HTTP, such as how to snoop on HTTP requests made by applications on your computer or phone

You know just the person who is contractually obliged to help.

----

Kate Kateberry trudges into the Steveslist office in the 19th century literature section of the San Francisco Public Library, ready for yet another day of changing the world and making Peter Thiel richer. Before she has a chance to start her morning ritual of scrolling through Twitter until 3pm, you sidle over to her desk and ask if she can give you a concise, yet detailed, yet real-world-relevant overview of the HTTP protocol. 

She is surprisingly enthusiastic. You wonder if you're paying her too much. She bounces over to the whiteboard. Before she can start, you ask if you can skip all the basics about `GET` and `POST` and so on, since you already know all that stuff. You might know that `GET` and `POST` exist, Kate replies, but do you know *why* they exist? Why does HTTP bother with them? Why doesn't HTTP keep things simple and just have a single type of request?

You aren't sure. This, says Kate, is just one of the many nuances that we will learn about today.

You start taking notes. You use your phone because you don't know where your laptop is. You know that it's somewhere in your apartment but you just can't find it. You aren't sure if you're allowed to claim on your insurance in this kind of situation. Anyway.

----

### What is HTTP? begins Kate

HTTP stands for *Hyper Text Transfer Protocol*. HTTP was originally designed by Tim Berners-Lee at the dawn of the internet, and it's the protocol that underlies most of the web. Whenever you use a web browser or an app to load a website, image, or any other kind of data, there's a very good chance that you're doing so using HTTP.

HTTP is a protocol. A protocol is a set of rules that allows two or more entities to exchange information. In order to better understand protocols, let's consider a more everday example: the set of rules for talking on a two-way radio (or walkie-talkie). When talking in a leisurely, face-to-face setting, most humans can use subtle physical and audio cues to help navigate conversational administration, such as whose turn it is to speak and whether both parties have been understood. However, on a crackly, audio-only walkie-talkie much of this human intuition is useless. In this context people become more like computers, and need fixed rules to help structure and clarify their conversations. This is the job of the two-way radio protocol.

The two most well-known rules of the protocol, commonly used and mis-used in spy films, are:

* When you have finished speaking say "over"
* When you are about to terminate the call say "out" (despite what every character in every movie always does, you should never say "over and out")

The protocol has plenty of other rules too:

* If you need the other person to repeat what they just said, say "say again".
* Start every call by identifying yourself, for example "Zulu619, this is TangoFoxtrot83".
* Do not interrupt when the other person is speaking. If there is an emergency then say "emergency" and describe the problem.
* When you want to say individual letters (eg. "the target's name is Smith, spelt S, M..."), use the NATO phonetic alphabet (eg. "the target's name is Smith, spelt Sierra, Mike...")

Humans can use these the radio protocol to communicate clearly with other humans over the airwaves. Similarly, pieces of software can use the HTTP protocol to communicate clearly with other pieces of software over computer networks like the internet (technically HTTP can also be used over mediums other than a computer network, but this is rare and isn't important to us right now). If a properly-written HTTP server receives a correctly-formatted HTTP request then it will be able to understand and parse it, and will respond with a correctly-formatted HTTP response. A system that is capable of using the HTTP protocol is often described as *implementing* it. Arguably an appropriately-trained person implements the two-way radio protocol, but no one would ever say that in real life.

HTTP is not itself a piece of software; it's a set of rules that are implemented by other pieces of software. You don't "download", "install", or "run" HTTP. The closest thing that HTTP has to physical form are the Internet Engineering Task Force's (IETF) Requests For Comments (RFCs) that define its specification. They are surprisingly readable.

HTTP is an open, non-proprietary standard. This means that it is not owned or copyrighted by any organization, and anyone may create devices and programs that use HTTP. Products created by warring companies can use HTTP to communicate with each other; Apple phones can talk to Microsoft servers running on Amazon hardware. Without common standards you would expect each company to devise their own protocol for talking to their devices. Some of these protocols might even be considered intellectual property that other companies or hobbyists were forbidden to use. The internet would be a more complex and worse place.

### Where is HTTP used?

HTTP is most prominently used to load webpages. Whenever you open a site, your web browser makes HTTP requests in order to retrieve the site's HTML, JavaScript, images, videos, and other resources. Browsers can sometimes use other protocols too; for example, the *web sockets* protocol is useful for realtime tasks like collaborative document editing. However, most resources in most webpages are loaded using HTTP.

HTTP is also often used to query APIs. "API" stands for Application Programming Interface, although no one ever writes that out in full. The acronym is used in many different situations, but in the context of the internet an API is an interface that allows programs to communicate with an online service. For example, the Facebook API allows programmers to write code that talks to Facebook, instead of requiring a human to click around a browser or an app. APIs may allow programmers to retrieve data (like a list of a user's friends) or write new data (like creating a new status update).

[TODO-PIC] of using an API

Since web browsers are centred around HTTP, websites and their servers have no choice but to use HTTP to communicate (apart from the odd exception alluded to above). However, if an API or other service is not designed to be accessed from a browser (such as an online dating mobile app) then its creators can have that service use any protocol that they like. They can even invent their own. Nonetheless, many still choose to stick with HTTP. HTTP is simple, lots of libraries already exist to support it, and lots of programmers are already familiar with it. As a rule of thumb, if HTTP will do the job for your application, there's a good chance that you might as well use it. On the other hand, some applications, such as online multiplayer games, have particularly tight speed or data constraints that a general-purpose protocol may not be able to meet. Developers of these applications may decide to spend the time and effort designing their own protocol, optimized for their own specific usage.

[TODO-PIC] of diff between HTTP and your own protocol

There's no difference in the fundamental nature of HTTP whether it is used by a browser, an app, or a custom program that queries an API. HTTP requests are structured the same, irrespective of the type of program that sent them. The data in the HTTP responses might take a different form depending on the context; browsers will often ask to get back HTML webpages, whereas apps and custom programs might require structured data like *JSON*. Even this is a poor dividing line, however, since webpages are increasingly structured as full-featured "web apps" that query the same APIs as mobile apps and custom programs. Nonetheless, in all cases the protocol is still fundamentally HTTP, and it matters little what types of device is on either end of the exchange.

### Where does HTTP fit in?

There's a lot that goes on behind the glib statement "the client sends an HTTP request to the server". How does the HTTP request know where to go? How does the server know where the request came from? How is it possible that an entire movie can be transmitted across oceans and under cities in seconds without losing any of its fidelity or emotional power?

The rules of HTTP are only responsible for specifying the structure of the messages that clients and servers exchange with each other. However, HTTP doesn't say anything about *how* these messages should travel between clients and servers. HTTP works at the level of an application, and so is often described as an *application-level protocol*.  Getting HTTP messages safely from sender to receiver is the job of *network-level protocols*, and almost all HTTP requests are exchanged using a combination of TCP (Transmission Control Protocol) and IP (Internet Protocol). The responsibilities of the network and application layers are well-separated: TCP/IP is responsible for moving data between computers, whereas HTTP is responsible for how that data is structured.

[TODO-PIC] of differen layers

As a user of HTTP you don't have to worry about the details of TCP/IP if you don't want to. You can use an HTTP library to send an HTTP request to `steveslist.com` or setup a server "listening on port 443", and let your operating system take care of the rest. If you're interested then you can learn about TCP/IP and the myriad others layers underneath and alongside HTTP - Ethernet, DNS, SSL/TLS, and many more. This will be useful and interesting. However, understanding HTTP only requires understanding only the structure, purpose, and sequencing of HTTP requests and responses. It is not essential, although it is useful, to delve into the layers below this. You can leave things at "the client sends an HTTP request to the server" and trust that the pieces of software that other people have built will do their jobs without you. This hiding of details is known as *abstraction*.

Abstraction is everywhere in the non-computerized world too; we just don't call it that. Think about the abstractions at work when you send a letter. The postal service is responsible for getting letters from A to B; you are responsible for the contents of those letters. You don't have to care how the postal service works after you drop off your letter; the postal service doesn't have to care what's in your letter. The details are hidden wherever possible, and the world is easier to think about as a result.

-----

Nonetheless, says Kate, understanding the lower-levels of a system can't help but improve your understanding of the higher-levels too. You may have found that you can get a long way without knowing much of the details of HTTP, let alone TCP/IP and beyond, but every new layer that you are familiar with will increase your appreciation of the whole. The goal of today's lecture is therefore to give a flavor of the details of HTTP, including seasonings that you won't find in core documentation or brief introductions.

I'm already here, you think, you can cut the sales pitch.

-----

### What does an HTTP exchange look like?

HTTP is a client-server protocol. "Client" and "server" are general purpose words that are widely used all over the field of computing. A server is a system that serves the needs of other systems, and these other systems are known as clients. A server sits around waiting for incoming messages from clients; servers don't pro-actively reach out to talk to clients (or at least if they do then they aren't acting as a server). When a server receives an incoming message it may perform an action of some sort, and the client and server may engage in an extended exchange of information. The server for `robertheaton.com` twiddles its thumbs waiting for HTTP request from a client, such as a web browser. When a client sends the `robertheaton.com` server an HTTP request asking for a resource like a webpage or an image, the server retrieves the appropriate data and sends it back in an HTTP response. It's not hard to come up with real-world analogies to this. A restaurant is a bit like a server; it offers food services, and it sits around waiting for customers (or simply, clients) to make use of these services. Restaurants don't provide food to customers unless they first receive an order.

[TODO-PIC] req/res of sitting around waiting

An HTTP client can be any device that needs to retrieve data from a remote server. It could be a web browser, a phone, a PlayStation, or a smart-fridge. An HTTP server can also technically be anything that is capable of listening for and responding to incoming HTTP requests. In practice servers are almost always big computers sitting in data centers. But if you were determined enough you could probably hook up a website that was powered by your smart-thermostat, and it is genuinely possible (if hard work) to run a fully-functional web server from a laptop in your front room.

[TODO-PIC] of phone ignoring incoming requests

In addition to being a client-server protocol, HTTP is also a request-response one. This means that every HTTP exchange consists of a single request from a client and (almost always) a single response from a server. After that, the HTTP exchange is over. If the client wants to request more information then it must send a brand new HTTP request.

It's possible to have a client-server protocol that is not also request-response. For example, the *web sockets* protocol is often used to power ongoing, realtime communication between a client (usually a web browser) and a server. This is useful for features like instant messaging. Web sockets still follows a client-server model: servers sit around waiting for connections from clients, and never initiate connections with clients. However, once a connection is established, both parties are free to use the connection to send as many messages as they want, when they want. They don't have to wait for a response, and they can continue to send messages even after they do receive responses. This is very different to HTTP's single request, single response model, and is exactly the behavior you want for an instant messaging application.

You can (and plenty of people do) still use HTTP to create realtime, instant messaging-style applications. However, because every HTTP conversation has exactly one request and one response, the client has to constantly "poll" the server, asking "Any new messages? Any new messages? How about now?" The more realtime you want your application to be, the more frequently the client must poll. This works, but puts a lot of load on the server. By contrast, the long-lived web sockets model allows a server to say "OK client, we've got a connection, it's not going away, chill out and I'll let you know when something new happens." Different protocols are useful for different things. Web sockets is good at bi-directional, ongoing communication, but HTTP is much more appropriate for downloading a large image file.

### A close look at an HTTP exchange

We've talked a lot about the uses and broad characteristics of HTTP; now we're ready to inspect the nuts and/or bolts of a real HTTP exchange. 

Let's look at an HTTP exchange in 6 steps:

1. The client builds an HTTP request containing the data that it wants to send to the server. An HTTP request is nothing more than a carefully structured block of text (we'll talk much more about this structure shortly), so this step is like the client writing a letter to the server before sending it in the next two steps.
2. The client opens a *TCP connection* with the server. You can think of a TCP connection as a pipe that the client and server send information to each other through. To open a new connection, a client sends a message (called a *TCP SYN packet*) to a server that is listening for incoming TCP connections. The client and server exchange a few more introductory formalities, and by the end the client and server have a reliable connection that they can use to send information to each other. The full details of TCP are a whole new story on their own (this description of an attack on TCP that I wrote might help[LINK]), but thanks to abstraction, you don't need to know about them if you don't want to.
3. The client uses the TCP connection that it opened in step 2 to send the server the HTTP request that it built in step 1.
4. The server receives the client's HTTP request. Because the server knows about the HTTP protocol and how HTTP requests are structured, the server can parse the request and understand what it means. Depending on the contents of the request the server might perform an action and/or retrieve some data.
5. Once the server has finished its work, it sends an HTTP response back to the client over the same TCP connection established in step 2. This response should contain either the data that the client requested, an explanation of any work that the server did (eg. "I successfully created a new message"), or an error message if something went wrong.
6. The client receives the HTTP response. The client parses the response and makes use of the data it contains. For a web browser this might mean displaying a web page; for a travel website this might mean saving flight details to its database.

This is how HTTP requests and responses are exchanged. Now we're ready to look at how these messages are structured.

### Overview of an HTTP request

HTTP requests are sent by a client to a server in order to initiate an HTTP exchange. They are made up of 5 components. We'll go through each in detail shortly, but in brief they are:

* HTTP method - a parameter that usually describes the type of action to be performed (examples include `GET`, `POST`)
* Request target - the resource that the request will act on (eg. `/about`, `https://google.com/search`)
* HTTP protocol version - the version of the HTTP protocol that the request uses. Most commonly 1.0, 1.1, or 2 (we'll talk about the differences between versions later, although they aren't particularly important for our discussions).
* Headers - metadata about the request (eg. cookies, the type of response expected, data about caching)
* Body - the main block of data (if any) that the client wants to send to the server (eg. the body of a message to be created, the details of an item to be purchased)

[TODO-PIC] - example HTTP request

Here's an example HTTP request that your browser sends when it wants to request the page `robertheaton.com/about` from my server:

```
GET /about HTTP/1.1
Host: robertheaton.com
User-Agent: curl/7.54.0
Accept: */*
TODO
```

Here's another HTTP request that an online business might send to the Stripe API. This request is asking Stripe to charge $50 to the credit card of customer `cus_123456789`:

```
TODO
```

Remember, these aren't human-readable summaries of HTTP requests. They're the exact text (ignoring a few minor complications like encryption and HTTP 2) that a client sends to a server, and that a server parses and processes. Let's go through each component of these requests in more detail.

### Request target

The request target of an HTTP request describes the thing, or *resouce*, that the HTTP request acts on. It is usually a URL path, such as `/articles/123`. For example, an HTTP request with a target of `/post/123` is probably going to act on the post with ID 123. An HTTP request with a target of `/posts/456/comments` is probably going to act on the comments of the post with ID 456. An HTTP request with a target of `/images/profile_pic.jpg` is probably going to act on an image called `profile_pic.jpg`. However, these are just conventions, and a server is free to do anything it likes in response to any request for any target.

Sometimes the request target gives a hint about the action the HTTP request will perform, not just the thing it will perform that action on. For example, to pay an invoice using the Stripe API, a programmer sends a `POST` (see below) request to `/v1/invoices/:invoice_id/pay`[TODO-LINK] (notice the `/pay` at the end). However, often the target doesn't give any direct hints about the action that the request will perform. Sending an HTTP request to `/v1/invoices/:invoice_id` could either retrieve, update, or delete the target invoice. Which action is taken depends on the *HTTP method* (eg. `GET` or `POST` - see below).

Sometimes a request target refers directly to an actual file on the server. One way of running a website is to use a simple server program like the Apache HTTP server[LINK]. You can run Apache on a server and tell it to respond to HTTP requests by looking up the URL path of the request, relative to a root directory on the server's hard disk. For example, suppose that you configure the root directory to be `/var/www/html`. Then, if your server receives an HTTP request for `/images/pineapple.jpg`, the server looks to see if it has a file saved on its disk at `/var/www/html/images/pineapple.jpg`. If it does, it reads and returns the file's contents; if not, it returns a 404 error.

[TODO-pic] how apache works

Note that servers using this approach have to be careful to reject malicious requests for sneaky paths. For example, a request to `GET /../../secret-docs/passwords.txt`, if treated naively, could end up returning the file at `/var/www/html/../../../secret-docs/passwords.txt`. Since `..` means "go up a directory", this path resolves to `/secret-docs/passwords.txt`, which judging by its name is not a file that you want to expose to an attacker.

This simple approach of using paths to directly access a file system was more common at the dawn of the internet, when a website was nothing more than a collection of static files. Nowadays most websites are more complicated and contain mostly dynamic content. These websites run more intricate server software that dynamically generates webpages and other responses on the fly, instead of reading and serving pre-generated files from a server's hard disk.

### HTTP methods

Every HTTP request has an *HTTP request method*. Despite the name, a request method doesn't change anything about the manner in which a request is sent. Instead, a request method is simply a word at the start of the first line of an HTTP request. Its function is to inform the action that the request takes on the resource in the request target. There are many conventions around which HTTP methods should be used for which types of action. As we will see, these conventions provide useful hints to clients about how they should process requests and responses.

For example, `GET` requests usually ask for information about the request target. This information is contained in the response's *response body*, and the form that it takes depends on the context. `GET` requests for webpages that will be displayed in a browser should usually return HTML; requests to an API should usually return data in a more structured format such as *JSON* (more on which later). In a vacuum both are sensible ways to respond to a `GET` request, and it's up to the programmers writing the server software to return data in the most appropriate form.

There are 39 HTTP methods in total[https://www.iana.org/assignments/http-methods/http-methods.xhtml#methods], but if you're a normal person then you'll only ever see or use an absolute maximum of 9 of these[LINK to MDN]. Common methods include:

* `GET` - typically used to return information about existing resources. A `GET` request to `/articles/999` should return information about article 999.
* `POST` - typically used to create new resources. A `POST` request to `/articles` should create a new article.
* `PUT` - typically used to update existing resources. A `PUT` request to `/articles/987` should update article 987.
* `DELETE` - typically used to delete existing resources. A `DELETE` request to `/articles/333` should delete article 333. 

You may have noticed that I've been using a lot of weasel-words like "should" and "likely". This is because all of these "rules" about what different HTTP methods should do are only conventions and best-practices. One of the most common conventions is called REST (Representational State Transfer), which provides programmers with a broad framework for deciding which request targets and methods should cause which actions to be performed on their server. However, there's nothing stopping a programmer from having a `GET` request to `/home` delete a user's account, other than hopefully their colleagues' code review.

Indeed, HTTP request method conventions are routinely broken in situations where the payoff is deemed worth it. For example, consider "magic login links". Some websites don't login their users using a username/password form. Instead they ask a user to submit just their email address. The website generates a long, secret, random token, and saves it to its database alongside the user's ID. The website then sends the user an email containing a "magic login link", the URL of which contains the random login token:

`https://socialnetwork.com/login?token=3873278132h181432hs12d178924nc17982t1dgj9jg89`

When the user clicks on the link, the website reads the token from the HTTP request path and looks up the user associated with the token in its database. Because the token is long, secret, and random, the website concludes that the only way that the sender of this HTTP request could have known about it is from the email containing the magic login link. The website therefore concludes that the requestor is also the owner of the email account associated with one of its users, and so logs the requestor in automatically, without the need for a password.

[TODO-PIC]

Username/password forms are usually submitted using `POST` requests. This is because `POST` requests are conventionally used to create new resources, and logging in creates a new *session* on the website. However, when a user clicks on a link, including one in an email, their browser by default requests the URL as a `GET`. It is possible for a website designer to have browsers request link URLs using other HTTP methods (including `POST`), but this requires JavaScript or HTML forms, neither of which are allowed in emails. This means that, even though magic login links create a new session on the website, they have no choice but to use `GET`. This is a violation of the convention that `GET` requests should only ever return information, and should not "do" anything or cause anything to change on the server. But rules were made to be broken, and magic login links are generally seen as a convenient and reasonably secure way to authenticate users.

HTTP could have been designed without any concept of HTTP methods, with instead only a single "type" of HTTP request. In actual, real HTTP, you would expect to use the HTTP path `/articles` in order to both create and list articles. The server can tell whether the requestor wants to create or list articles from the request's HTTP method:

* Retrieve a list of all articles: `GET /articles`
* Create a new article: `POST /articles`

However, one can easily imagine a world without HTTP methods. In this world, all the information about the intent of a request would be contained in the URL path:

* Retrieve a list of all articles: `/articles/list`
* Create a new article: `/articles/create`

This approach would work. However, the conventions around the types of actions that HTTP methods should and shouldn't perform provide hints about the behavior of an HTTP request. These hints can be very useful, and would not be available in the alternative world without HTTP methods. Two such hints provided by HTTP methods are about a request's *safety* and *idempotency*.

#### Safety

It is useful for a client to know whether an HTTP request will be *safe*. A safe request is one that doesn't change any state on the server. It doesn't write any new data, and it doesn't update any existing data. All it does is read and return data. When a client makes a safe request, it can be confident that it won't cause anything to change on the server. 

Convention says that all `GET` requests should be safe (as should much rarer `HEAD`, `OPTIONS`, and `TRACE` requests), but that `POST`, `PUT`, and `DELETE` requests need not be. This is a useful property because it means that web crawlers can safely automatically click on links, and browsers can safely pre-load data like the top Google result for a search. So long as all the requests they send are `GET`s, the crawlers and browsers can be confident that they won't change the state of the server, and so won't cause any problems for website operators.

The fact that `GET` requests are safe is useful in other situations too. When you type a URL into your browser address bar and press enter, your browser always requests this URL with an HTTP `GET` request. Since `GET` requests should only ever return data, not mutate it, this means that typing a URL into your browser is always "safe". You can be very confident that visiting `https://facebook.com/account/delete` in your browser isn't going to accidentally delete your account. If there was only one type of HTTP request, you would have no such guarantee.

#### Idempotency

A request's HTTP method also indicates whether the request is *idempotent*. If a request is idempotent then this means that the effect on the server of sending it multiple times is the same as sending it once. This means that you can happily send the request as many times as you like without worrying that the server might double-perform the action that it requests. If you aren't sure whether an idempotent request was received and actioned, you can keep sending it until the server acknowledges it.

Requests that create a new resource of some kind will often not be idempotent. Consider a request that asks your online bank to transfer £100 to your friend. If you re-send the exact same HTTP request twice in succession then, unless the server has mitigations in place (see below), the server has no way to know whether this was an accident or whether you really do want to make a second transfer. The server has no good options: either it sends the second transfer and risks sending too much money if the request was a mistake, or it ignores the second request and risks sending not enough money if the request was deliberate.

This is why your browser will sometimes warn you when you refresh a webpage that was returned from a `POST` request.

[TODO-screenshot]

Your browser knows that, in order to refresh the page, it will have to re-send the `POST` request that generated it. Your browser doesn't know what the server will do when it receives this re-sent request. But, thanks to the hints afforded by HTTP request methods, it knows that `POST` requests may not be idempotent. It knows that a repeated request may cause a repeated action, and warns you that you might be about to re-trigger an action when you attempt to refresh the page. Without HTTP request methods, it would not be able to make this inference.

Applications can and should be structured so as to make more of their actions idempotent, reducing the opportunity for error. Where possible online shops shouldn't simply send themselves HTTP requests saying "customer ABC wants us to charge them £250". Instead their requests should say something like "customer ABC wants us to charge them for order number 123". If order number 123 has already been paid for then the application knows that they should ignore any subsequent duplicate requests. The first form of this operation is not idempotent, but the second one is.

Some APIs turn non-idempotent requests into idempotent ones by using *idempotency keys*. When a client makes an API request, they also generate and include a long, random string called an idempotency key:

```
TODO example HTTP req includeing idem key
```

If the client isn't sure whether a request succeeded (perhaps because of a bug in their program, or a problem with their network), the client should send the same request again *with the same idempotency key*. When the server receives an API request that includes an idempotency key, the server checks to see whether it has already processed a request from the user with that key. If it has not, it processes the request as normal and saves a record of the idempotency key that was used, along with the HTTP response that it returned. On the other hand, if the server has already seen the key, it knows that it should not process the identical request again. Instead, it returns the response that it returned previously and has saved in its database. By using this approach a client can make sure that it does not accidentally use an API to make the same non-idempotent request twice.

[TODO-pic] flowchart of idempotence key

For a real-world example of idempotency keys, see the Stripe API documentation[LINK].

Since `GET` requests are safe and don't change the state of the server, they should always be idempotent by default. Your browser will never warn you when you refresh a page returned from a `GET` request. Other types of requests may be intrinsically idempotent too, for example `DELETE` requests that delete a resource. If the resource hasn't been deleted, the server should delete it. If it has already been deleted, then it can sensibly ignore the request. The effect of sending the request multiple times is therefore the same as sending it once, which is the definition of idempotency.

An HTTP request's HTTP method and path describe the action that a request should perform. However, many actions also require additional parameters in order for the server to be able to carry them out.

### HTTP request parameters

Clients can't just say "create a new article"; they also need to provide the new article's title, body, tags, and so on. Read-only operations like "list all articles" may require parameters too, like a specific author, date, and so on. There are two main ways of passing parameters in an HTTP request: the *query string*, and the *request body*.

#### Query string

The query string is an optional section of a URL path, separated from the base URL by a `?`. For example, consider the following URL:

`https://newspaper.com/articles?author=Robert%20Heaton&year=2020`

The base of this URL is `https://newspaper.com/articles`, and the query string is `author=Robert%20Heaton&year=2020`. Query strings usually encode multiple parameters using a `key=value` syntax that looks like `?key1=value1&key2=value2`, although clients and servers are free to agree on any syntax that they like. We would expect that a `GET` request to `https://newspaper.com/articles?author=Robert%20Heaton&year=2020` will return all articles written by Robert Heaton in the year 2020.

`GET` requests should provide all their parameters in the query string. By contrast, `POST` or `PUT` requests usually pass their parameters in the HTTP *request body*.

#### Request body

A request body is a blob of arbitrary text that is attached to the end of an HTTP request. In our example of an HTTP request that creates a new article, the parameters of the new article would conventionally be found in the request body.

[TODO-PIC]

Just as for query strings, the HTTP protocol doesn't make any requirements about how a request body should be structured. Clients are free to send whatever data in whatever form they like. As always, however, there are conventions and standards that most applications make use of. For example, many modern APIs require parameters to be sent using *JavaScript Object Notation*, more commonly known as JSON.

#### JSON

JSON is a *serialization format*, which roughly means that it's a way to represent data in a form that can be easily shared. JSON is by no means the only serialization format available. Older APIs might use *XML*; newer ones might use *protobuf*. You could even invent your own, although I wouldn't recommend that you use it in production.

JSON looks very much like the maps, dictionaries, lists, and arrays that you might use when you're writing a program. For example, suppose that you want to send an API request to create a new article. The API's documentation will tell you how to structure the JSON request body that you send it, including the properties that it needs (such as `title`, `body`, `tags`, and `author`). One reasonable specification might call for a request body that looks something like this:

```json
{
    "title": "An advanced beginners guide to APIs",
    "body": "TODO",
    "tags": ["programming", "advanced-beginners"],
    "author": {
        "first_name": "Robert",
        "last_name": "Heaton"
    }
}
```

All JSON is also valid JavaScript code - you can copy and paste a blob of JSON into a JavaScript program and it will immediately understand it. However, it is not at all JavaScript-specific. Almost every language has standard libraries for parsing JSON and converting it into that language's own internal data structures. For more information on JSON standards, see json.org[LINK].

#### HTTP headers

TODO




HTTP requests are only half of the story. We also need to consider HTTP responses.

------

Your thumbs ache from two hours of frantic note-taking. After the first fifteen minutes everything was typoed nonsense but you kept going because you didn't want to look like you'd lost interest.

Any questions so far? asks Kate. You don't want to seem stupid so you say no.

------

### HTTP responses

When a client sends a server an HTTP request, the server parses the request, performs an action, then sends an *HTTP response* back to the client. This response should tell the client whether their request succeeded and return any data that the client requested. HTTP responses are structured slightly differently to HTTP requests, but they do look similar and share some features. HTTP responses are made up of 3 main components:

* Status line - a standardized status code that describes whether or not the request succeeded. Status codes are written as a number, followed by a human-readable description of the status code (eg. `200 OK`, `404 Not Found`).
* Headers - metadata about the response. Response headers use the same structure as request headers, but their contents are typically different (eg. new cookies that the client should set, the filetype of the response)
* Body - the main block of data that the server returns to the client. Might be a webpage, an image, some JSON data, a JavaScript file, or something else entirely.

It's possible that if something goes seriously wrong (eg. the server literally explodes half-way through processing the request, or the client loses its internet connection) then a client might not receive an HTTP response to its HTTP request. However, this should be rare. HTTP is a request-response protocol, and in general every request should receive exactly one response.

#### Example HTTP response

Here's an HTTP request to `example.com`:

```
GET / HTTP/1.1
Host: example.com
Accept: */*
```

And here's an HTTP response to that request. You can see the status line (`HTTP/1.1 200 OK`) indicating that the request succeeded, then the headers (some of which I've snipped out for brevity), then a newline, and finally the HTTP body containing the HTML that is displayed by your browser when you visit `example.com`:

```
HTTP/1.1 200 OK
Age: 601062
Cache-Control: max-age=604800
Content-Type: text/html; charset=UTF-8
Date: Fri, 13 Nov 2020 08:59:45 GMT
Etag: "3147526947+ident"
Expires: Fri, 20 Nov 2020 08:59:45 GMT
Last-Modified: Thu, 17 Oct 2019 07:18:26 GMT
Content-Length: 1256

<!doctype html>
<html>
<head>
    <title>Example Domain</title>

<...snip...>
```

Let's look at the components of an HTTP response in more detail.

#### HTTP response codes

An HTTP response code (also often called its *status code*) is a number that tells the client what happened to their request. If a request failed, the response's status code describes the broad category of what went wrong. When written out, numeric response codes are often followed by a string describing the code to make them easier for humans to read (eg. `404 Not Found`). Some of the most common codes are are:

* `200 OK`: The request succeeded
* `400 Bad Request`: The server could not understand the request due to invalid syntax or parameters
* `403 Forbidden`: The client is not allowed to access the content 
* `404 Not Found`: The server can not find the requested resource. You've probably seen this in your browser many times. For an example, see eg. `https://robertheaton.com/THIS_PAGE_DOESNT_EXIST`[TODO]
* `500 Internal Server Error`: The server has encountered a situation it doesn't know how to handle. Often suggests that the server's code threw an exception while processing the client's request.

Response codes come in five groups. Groups are often described using a notation like `1xx`. `1xx` means the group of status code that begin with a `1`. This covers all numbers from `100`-`199`, although there are far fewer than 100 HTTP status codes that begin with a `1`.

* `1xx`, Informational responses: an interim response sent while a request is being sent. For example, `100 Continue` indicates that the server has received part of the client's request and nothing has gone wrong yet. You will rarely have to deal with `1xx` responses.
* `2xx`, Successful responses: the request succeeded. The exact response code may give some extra color.
* `3xx`, Redirects: a redirect status code means that "further action needs to be taken by the user agent in order to fulfill the request" (taken from the HTTP spec[LINK]). A redirect usually tells the client that the resource has moved to a new URL, and that the client should make a new request to that URL. For example, I've set up `https://robertheaton.com/redirect-example` to return a `3TODO` response that tells the client to retrieve its requested resource by making a new request to `https://robertheaton.com/about`. Web browsers usually follow redirects automatically - try visiting the above link to test this out[LINK].
* `4xx`, Client errors: there is a problem with the request because the client did done something wrong (eg. invalid parameters, non-existent resource)
* `5xx`, Server errors: the server has done something wrong (for example, there was a bug in the server code that threw an exception) and the server is unable to complete the request.

You can read the full list of response codes on MDN[LINK-https://developer.mozilla.org/en-US/docs/Web/HTTP/Status].

#### HTTP response body

The HTTP response body contains a blob of text that answers the client's request. Just like with request bodies, the HTTP protocol places no restrictions on how a response body should be structured. The body might contain a webpage, an image, an Excel spreadsheet, or anything else. Textual information like a webpage or JSON is typically sent as human-readable text. If you intercepted the HTTP response, you could probably read and understand this text.

[TODO-PIC] of intercepting HTML

Files like images are sent as a string of bytes that encode their contents, in the exact same way that they are stored as a string of bytes on a hard-disk.

[TODO-PIC] of intercepting pic

A sensible structure for a response body will depend on the context of the request. We know already that a `GET` request to the path `/articles/123` should return some sort of representation of article number `123`. If `/articles/123` is a website that is intended to be viewed in a browser then the represenation should likely be in the form of HTML:

```html
<html>
    <head>
        <title>Article #123 - BIG NEWS</title>

...etc...
```

On the other hand, suppose that `/articles/123` is actually an API endpoint used by programmers inside data analysis companies to retrieve news articles for linguistic analysis. In this case, the response should likely contain a representation of the article in the form of a serialization format like JSON:

```json
{
    TODO
}
```

A company might want to allow its users to view their data through both a webpage and an API, depending on their use case. To achieve this they might have two sets of HTTP endpoints; one for browsers to view articles as HTML (eg. `/articles/123`), and another for programs using the API to view articles as structured data (eg. `/api/articles/123`).

---

It’s 6pm and the library is closing. A librarian tries to ask you to leave. Kate says no and the poor guy clearly doesn't know what to do. He wasn't trained for this.

---

### HTTP 1.x vs HTTP 2

TODO

TODO: equivalent of below section but for HTTP servers

### Sending HTTP requests from your own code

Every day we all send hundreds, possibly thousands of HTTP requests from our web browsers and other applications. But how can you harness the power of the internet in your own programs? How can you write code that contacts an API, scrapes a webpage, or downloads an image? How can you write code that makes HTTP requests?

Almost all mature, high-level programming languages have libraries that help construct and send HTTP requests. You give a library the parameters of your request, such as the request target, the HTTP method, and the target domain or IP address. The library turns these parameters into a well-structured HTTP request, sends the request to your desired destination, and parses the response. This means that you don't have to remember any of the finnickier structural details that we've discussed so far. You don't need to remember whether the HTTP method goes before or after the URL, and you don't have to remember whether headers should be sent in any particular order. You certainly don't have to remember or even know anything about how TCP networking works. This is *abstraction* at work again, reducing the surface area of what you need to think about.

For example, the Python `requests` library allows you to make HTTP requests by writing code that looks like this:

```python
import requests

requests.get("TODO")
```

Libraries like `requests` provide functions and arguments for every part of the HTTP specification: adding parameters, using different HTTP request methods, reading headers, following redirects, uploading files, and all the rest. 

```python
import requests

# Send parameters in a POST request body
r = requests.post(
    'https://httpbin.org/post',
    data={'key':'value'},
)
# Set HTTP request headers
r = requests.get(
    'https://api.github.com/some/endpoint',
    headers={'user-agent': 'my-app/0.0.1'},
)
# Upload a file
r = requests.post
    'https://api.github.com/some/endpoint',
    files={
        'file': open('report.xls', 'rb'),
    },
)

# Raise an exception if the request failed
# (status codes 400 and up indicate an error)
if r.status_code >= 400:
    raise Exception(f"Error sending request: {r.status_code} {r.text}")
```

`requests` has extensive documentation[LINK] that shows many more example uses.

Practice writing programs that makes HTTP requests; it can be very gratifying to successfully communicate with another server and make use of the data that it provides. Search `http client library $YOUR_LANGUAGE`, and read the documenation. Try writing a program that sends and retrieves data to and from an API, for example a bot that pulls the weather forecast for your home town from the [OpenWeatherMap API](https://openweathermap.org/api) and uses the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) to Tweet it. Or you could build a web scraper that politely scrapes information from your favorite websites (search `how to write a web scraper` for inspiration), and publishes it to Instagram (or whatever).

HTTP client libraries are powerful, but sometimes you just want to fire off a quick HTTP request without having to write a whole program. In these situations you may turn to a command line tool like *`curl`*.

#### Sending HTTP requests from the command line: `curl`

Suppose that you want to get a quick and dirty idea of what data a request returns. Maybe you want to see whether passing a new header changes the response you get back from a server, or whether you're even able to connect to a particular URL. If you don't want to setup and write a whole program then you might find `curl` useful instead.

`curl`[LINK] is a command line tool available on most operating systems, including Linux, macOS, and Windows. It allows you to send HTTP (and other protocol) requests from the command line by running commands like this:

```
$ curl https://robertheaton.com
```

This command will send an HTTP request to `https://robertheaton.com` (as a `GET` request by default) and print the response. `https://robertheaton.com` is designed to be viewed as a webpage, so a request to it returns HTML:

```
$ curl https://robertheaton.com
<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Robert Heaton</title>
<...snip...>
```

You can add the `-v`, or `--verbose`, flag in order to tell `curl` to print information about the low-level actions it is performing. This can be particularly helpful for debugging when something goes wrong, or for learning about the details of HTTP and networking. You can apply `-v` up to three times for maximum detail:

```
$ curl https://robertheaton.com -vvv
* Rebuilt URL to: https://robertheaton.com/
*   Trying 104.18.33.191...
* TCP_NODELAY set
* Connected to robertheaton.com (104.18.33.191) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
<...snip...>
```

Try running `curl https://robertheaton.com -vvv` from your command-line now. If your shell tells you that you don't have `curl` installed, look up how to install it for your operating system[TODO-LINK]. Use `curl` to query other URLs, see what you get back, and keep `curl` in mind for when you want to have an experimental poke at an HTTP server.

### Sending artisanal HTTP requests by hand

HTTP libraries, `curl`, and web browsers all take care of assembling and sending HTTP requests for you. You will almost certainly go through your entire life without ever needing to construct an HTTP request yourself. This is not cheating; it frees you up to focus on the parts of your systems that matter.

That said, it can still be instructive to hand-roll your own requests once or twice. We've said already that the HTTP requests and responses that we've been looking at aren't just human-readable summaries of more complex structures; they are literally the data that a client sends and a server parses. There are complications to this slightly-too-tidy model; the client may use SSL/TLS to encrypt the data before sending it (see the section on HTTPS below), and the newer HTTP/2 protocol compresses data before sending it. But it is still straightforward, if laborious, to fashion and send your own unencrypted HTTP 1.x requests by hand.

To give this a go you'll need a tool like *netcat*[TODO-LINK]. Netcat is a command line tool that you can use to establish a raw TCP connection with a server. Making a TCP connection between a client and a server is like constructing a drainpipe between the two machines that they can send messages through. Once the drainpipe is constructred, TCP doesn't say anything about what the form of those messages should be. That's the job of an application-level protocol like HTTP.

This means that you can use a tool like netcat to open a raw TCP connection to a server, then type out and send your own hand-rolled HTTP requests down your TCP drainpipe. Try this out now. First, install netcat[LINK]. Then use netcat to open a TCP connection to `example.com` - which is a real domain - by running:

```
$ nc example.com 80
```

The `80` tells `netcat` to connect to `example.com` on *port 80*. Ports are a networking-level concept that the HTTP protocol doesn't have to care about much, meaning that right now we don't care either. This command will open a TCP connection with `example.com` and then pause. Once the connection is established, whatever text you type into your command prompt will be sent to the target server over TCP.

Since you have complete control over the data that is sent and are not constrained by the HTTP protocol, you could pass the server non-HTTP nonsense. However, the server will respond politely saying that it has no idea what you are talking about:

```
$ nc example.com 80
how now brown cow

HTTP/1.0 501 Not Implemented
Content-Type: text/html
<...snip...>
```

On the other hand, if you send the server a valid HTTP request, it will receive and process it just as if it had been generated by a program:

```
$ nc example.com 80
GET / HTTP/1.1
Host: example.com

HTTP/1.1 200 OK
Accept-Ranges: bytes
Age: 275740
Cache-Control: max-age=604800
<...snip...>

<!doctype html>
<html>
<head>
    <title>Example Domain</title>
<...snip...>
```

In the above example the first two lines, beginning with `GET` and `Host` respectively, are typed in manually by you. All following lines (from `HTTP/1.1...` onwards) are returned by the server. From the `200 OK` status line you can see that the server received and understood your manually-crafted but well-formed request. It processed your request, and returned the HTML for the webpage that lives at `example.com`. You will never, never need to do this in real life, but trying it out once or twice can be illuminating and can help demystify computer networking.

We've now seen several tools that you can use to make HTTP requests of your own, and it's been perfectly interesting and productive. However, in my opinion the real fun is in snooping on HTTP requests sent by other programs running on your computer.

### How to see inside HTTP requests

I have three common reasons that I find myself wanting to spy on a program's HTTP requests. First is because I want to see whether the program is smuggling out any of my personal data (see my posts "Stylish browser extension steals all your internet history"[LINK] and "Wacom drawing tablets track the name of every application that you open"[LINK]). Second is because I want to reverse-engineer undocumented APIs used by websites and smartphone apps, and use these APIs to write my own programs that interact with servers in ways that their creators may have never intended (see my post "How Tinder keeps your exact location (sort of) private"[LINK]). Third is because I want to - if the owner allows it - probe a service for security vulnerabilities (see my posts "Remote code execution vulnerability in KensingtonWorks mouse manager"[LINK] and "Another RCE vulnerability in KensingtonWorks"[LINK]).

Many desktop and mobile apps applications communicate with a central server in order to load, backup, retrieve, and exchange data. Zoom makes video calls. Evernote, a note-taking app, saves your data to the cloud. The Apple App Store loads lists of the latest releases from the Apple servers. These applications are free to talk to their central server using any application-level protocol that they want, or even to invent their own. Nonetheless, many applications stick with HTTP because it's simple; lots of libraries already exist to support it; and lots of programmers are already familiar with it.

Two common tools for nosing around in other applications' traffic are an *HTTP proxy* and a *network packet analyzer*. Both are powerful yet accessible ways to find out what your computer is doing behind the scenes.

#### HTTP Proxy: Burp Suite

Suppose that you've identified an application that you suspect might be funneling your personal data back to a central server. You want to inspect the HTTP traffic that it is sending in order to see what data it contains. The first tool to try is a *man-in-the-middle HTTP proxy* (hereafter shortened to *HTTP proxy*), like *Burp Suite*[LINK].

An HTTP proxy is itself an HTTP server, usually (but not necessarily) running on the same machine as the target application. Once you've set up your proxy, you ask the target application to send its HTTP requests to your proxy server, instead of directly to the intended destination. When your proxy receives a request it first logs its contents, and then forwards the request on to the server to which it was originally meant to go. The server sends its HTTP response back to the proxy. The proxy logs the response and forwards it back to the originating application.

[TODO-PIC]

The proxy's logfiles show you the HTTP requests that your applications send and the data they include. When I discovered that the popular browser extension "Stylish" was sending the URL of every single website[LINK] that I visited back to its owners, it was because I was running an HTTP proxy on my computer in order to snoop on the activity of an unrelated application. However, while reading my proxy's logs I noticed some strange HTTP requests being sent to a suspicious domain. A few minutes of investigation later I had worked out what Stylish was up to and had already begun an indignant blog post.

You can also use your proxy logs to work out how your target application structures its requests to its central servers. You may be able to determine the structure of the URLs the application sends data to, what serialization format it uses for its parameters, and what keys and values these parameters contain.  The information that you discover may allow you to reverse-engineer the APIs for these central servers, which in turn may allow you to write programs to send your own requests to the servers in order to automate tedious and repetitive tasks like texting your family. Your proxy may provide power features to help with your investigation, such as allowing you to edit parameters in the request or response before forwarding it, or to save a request and repeatedly re-send it.

Proxies only work if you can persuade the target application send its HTTP requests via your proxy, instead of to their real, intende destination. Some applications provide their own configuration to help with this; others allow you to select a proxy at the operating system level. However, some ignore all requests to send their requests via a proxy. Applications aren't obliged to offer any proxy configuration options, and they aren't obliged to obey the operating system configuration. A technique called *DNS spoofing* may still trick the application into using your proxy, but it can be fiddly and is not guaranteed to work.

It may seem like a good idea for companies to have their products ignore proxies at all costs. Even if a company doesn't have any sneaky data exfiltration to hide, surely it makes their lives easier if no one can reverse-engineer their private APIs or probe their application for vulnerabilities?

However, proxies are not only used by nosy programmers. For example, large, security-conscious companies may force all traffic from their employees' laptops to travel through a proxy in order to detect and block malicious traffic. To ensure compliance, the company may prevent any traffic from leaving the laptop unless it goes via their proxy. In this situation, if an application ignores the operating system's proxy settings then it may find itself unable to contact the outside world at all.

Snooping on programs running on your computer isn't the only use for HTTP proxies. You can also use them to investigate apps running on your smartphone. To do this, you connect your laptop and your smartphone to the same wi-fi or wired network, and then configure your smartphone to send its HTTP traffic via a proxy server running on your laptop. Burp Suite has extensive documentation on doing this[TODO-LINK].

[TODO-pic] of sending phone data via laptop

This allows you to see what data your mobile apps are sending back home (which is almost always a *lot*), and reverse-engineer the APIs that they use. You may have chatted with someone on Tinder who seemed real and attractive enough, until it slowly became apparent that they were a robot. The robot's controller almost certainly used an HTTP proxy to reverse-engineer the Tinder API, and now uses this knowledge to write programs that make automated matches and send automated messages.

[TODO-pic] of a robotic Tinder account

I would strongly recommend setting up an HTTP proxy and playing around, especially since the community edition of Burp Suite is free[LINK]. Setting up an HTTP proxy teaches you a lot about HTTP, networking, and SSL/TLS encryption (more on which below). It can lead you to some newsworthy discoveries about data privacy, and maybe allow you to cause some light-hearted automated mischief. For inspiration and examples of how I've used HTTP proxies to unmask malfeasance and mess with my mates, see "Stylish browser extension steals all your internet history"[LINK], "Wacom drawing tablets track the name of every application that you open"[LINK], and "Fun with your friend's Facebook and Tinder sessions"[LINK].

[TODO-PIC] of proxy blocking egress

If you find yourself trying to probe an application that isn't respecting your HTTP proxy, or you just want to see a lot more detail about what's happening on your network, you can turn to a *network packet analyzer*.

#### Network Packet Analyzer: Wireshark

HTTP proxies work at the HTTP layer. This means that they understand and are deeply integrated with the structure of HTTP requests and responses. They can provide HTTP-specific power-features when analyzing cooperative HTTP-based applications, such as intercepting a request and allowing you to edit it before forwarding it on. However, HTTP proxies are useless for inspecting applications that won't send their traffic through the proxy or that don't even use HTTP.

An alternative way to spy on your programs is to use a lower-level tool called a *packet analyzer*. Packet analyzers look at traffic at the network level. This means that they see and show you all the gory details of every byte that you computer sends out over a network. This includes everything from HTTP application data all the way down to the ethernet control frames that your computer exchanges with your wi-fi router. Even if an application won't co-operate with your HTTP proxy, if it wants to send data over your network, your packet analyzer will see this data as it leaves your machine.

[TODO-pic] diff between proxy and packet analyzer

One popular packet analyzer is called *Wireshark*[LINK]. Since Wireshark doesn't inject itself into the path that data takes out of your machine, it can't provide the same kind of tools to edit requests and responses that proxies like Burp Suite can. However, Wireshark isn't completely ignorant. It understands the structure of a wide range of different protocols, not just HTTP. It uses this knowledge to reconstruct raw bytes into human-readable summaries at the different layers of the stack. For example, it can take the ethernet frames that your computer sends to your router, reconstruct them into TCP/IP packets, then reconstruct the TCP/IP packets into HTTP requests and responses. This allows it to display the data it captures very usefully in its UI, and allows you to filter traffic using queries like `http.method == GET TODO`. If it didn't understand how to roll up raw bytes into different protocols, all Wireshark would be able to show you is a structureless, meaningless stream of numbers.

[TODO-pic] screenshot of Wireshark

Packet analyzers see everything, but they can't edit or block traffic in the same way that proxies can. They also can't decrypt SSL/TLS encrypted traffic (which we'll talk about more in the next section). This is because the SSL/TLS protocol is so robust. The point of SSL/TLS encryption (and any other useful encryption applied to data sent over a network) is to prevent eavesdroppers from spying on your traffic. SSL/TLS has the incredible property that even if an attacker watches every single packet of data that a client and a server exchange, *including the packets in which they negotiate and exchange an encryption key*, if that data is SSL/TLS encrypted, the attacker won't be able to decrypt it.

[TODO-pic] exchanging data but not decrping

Packet analyzers are only watching from the sidelines. From the point of view of decrypting SSL/TLS traffic, it's barely relevant that they run on the same computer as one of the participants in the encrypted conversation. In a very real sense they are spying on your traffic from exactly the same kind of vantage point as an attacker who has hacked into your network. If a packet analyzer were able to decrypt encrypted traffic by watching network packets, so could an evil-doer listening on your coffee shop's wi-fi. Instead, the best that Wireshark can do is to show you the garbled, encrypted bytes and say "there might be something interesting in here but unfortunately I can't tell you what."

[TODO-PIC] of TLS traffic in Wireshark

By contrast, because a proxy inserts itself in between the client and the server, it is directly involved in the encryption process. This means that it is able to decrypt and read the traffic. The proxy negotiates an encrypted connection with the client, and a second encrypted connection with the destination server. When the client wants to send data to the server (and vice versa), the client sends it over its encrypted connection to the proxy. The proxy decrypts the data, logs it so that you can inspect it, then re-encrypts it and sends it along its second encrypted connection with the destination server. This process is known as a *man-in-the-middle*.

[TODO-PIC] of MITM

Download Wireshark and set it running. The output may initially be overwhelming: Wireshark sees *everything*. To filter it to only show HTTP requests and responses, use the filter `TODO`. I made extensive use of Wireshark in my security vulnerability investigation "Remote code execution vulnerability in KensingtonWorks mouse manager"[LINK].

We've spoken about how proxies and packet analyzers do and don't deal with encryption. Now let's talk about how this encryption works.

### HTTPS

HTTP requests are almost always sent over a computer network. Unfortunately the world is a squalid and threatening place, doubly-so on computer networks. You should assume that all networks are insecure, and that all traffic sent over them may be snooped on or tampered with. When your browser sends a HTTP request to Facebook containing your username and password, this request may be seen by people on your coffee shop wi-fi, your ISP, your government, a foreign government that has hacked your ISP, or anyone else who has managed to insert themselves somewhere in the mazy route that your data takes from A to B.

Even though you should assume that every HTTP request you send might be intercepted, this doesn't mean that you should smash your phone and cut off your broadband. Instead, it means that you should make sure that every important HTTP request you send is *encrypted*. The current standard encryption protocol is called SSL/TLS, which stands for "Secure Sockets Layer/Transport Layer Security". The product of encrypting HTTP traffic with SSL/TLS is known as HTTPS; the "S" is for "secure". HTTPS is becoming so ubiquitous and straightforward to implement that not using it is commonly considered a security vulnerability. TODO - if an attacker intercepts encrypted traffic then they can't read it.

As well as keeping HTTP traffic safe from eavesdropping and tamperering, SSL/TLS/HTTPS also allow a client to verify the identity of the server that they are talking to (and, more rarely, vice versa). When a client attempts to negotiate an encrypted connection with a server claiming to be `facebook.com`, the client will require the server to present an SSL/TLS *certificate*. The certificate must attest that the presenting server is the real controller of `facebook.com` and must be *cryptographically signed* by a trusted third-party called a *certificate authority* (CA). A CA should only issue a signed certificate for a domain once it has verified that the recipient does indeed own and control that domain. Certificates allow a client to be confident that the server they are talking to is the real `facebook.com` or `gmail.com`, and not an attacker pretending to be one of these services. This is important because it doesn't matter how mathematically bulletproof your encryption algorithm is if you exchange your encryption keys with a hacker. I've written much more about certificates in my post "How does HTTPS actually work?"[LINK].

We said a paragraph ago that "you should make sure that every important HTTP request you send is encrypted". However, you don't directly get to decide this. Your web browser will encrypt its conversations with any server that offers SSL/TLS, but if a server does not offer SSL/TLS then you can't unilaterally encrypt your requests and expect the server to figure things out. Instead your browser will send its requests in unencrypted plaintext and probably display a sad open padlock in the address bar.

[TODO-PIC] screenshot of padlock

Being pragmatic, it's probably safe enough to browse someone's personal blog over plain HTTP, but if your online bank doesn't offer HTTPS then you should find yourself a new bank. Similarly, the applications that you use on your computer and phone "should" encrypt communications between your device and their servers. However, if they don't then you'll probably never know unless you go snooping and you don't have any options other than accept the risk or stop using the application.

On the better-behaved side, many websites automatically redirect unencrypted HTTP requests to their HTTPS equivalent using the 301 HTTP status code. For example, try using your browser to visit `http://robertheaton.com` (note the `http`, not `https`, at the start of the URL). You will be redirected to the equivalent HTTPS URL at `https://robertheaton.com`. You can also make this same request to `http://robertheaton.com` using `curl`, and see the raw HTTP redirect response:

```
$ curl http://robertheaton.com
<...snip...>
HTTP/1.1 301 Moved Permanently
Location: https://robertheaton.com/
<...snip...>
```

Many APIs will only accept HTTPS requests, and reject unencrypted HTTP:

```
$ curl http://app.asana.com/api/1.0
The Asana API can only be reached via HTTPS
$ curl http://api.stripe.com
{
  "error": {
    "message" : "The Stripe API is only accessible over HTTPS.  Please see <https://stripe.com/docs> for more information.",
    "type": "invalid_request_error"
  }
}
```

We've already seen how the network layer and the application layer are cleanly separated from each other. The SSL/TLS encryption layer is similarly separated from both these layers too. Adding SSL/TLS encryption to an HTTP conversation doesn't change anything about the HTTP layer. When sending an HTTPS request, the client opens a TCP connection with the server and forms its HTTP request exactly as normal. Then, instead of sending the request straight off to the server, the client first passes it through an encryption algorithm. Using the algorithm first requires some negotiations with the server - carried out over the existing TCP connection between the client and server - in order to agree on an encryption key. Finally the client sends the now-encrypted request to the server.

[TODO-PIC]

When the server receives the encrypted request it starts by decrypting it to recover the original plaintext HTTP request. It can then process the request just like a plaintext HTTP request, without having to care that it was ever encrypted. When the server comes to send an encrypted HTTP response back to the client, the exact same process is followed but with roles switched.

I've written much more about the details of how HTTPS works in my posts "How does HTTPS actually work?"[LINK] and "HTTPS in the real world"[LINK], in which we go deep into the details of TLS/SSL and public key cryptography.

----

FINISH FINISH CONCLUSION

You ostentatiously start converting your notes into flashcards, or at least you pretend to, since your notes really do read like a Mark Rothko painting by this point. You start explaining the benefits of flashcards and spaced repetition to Kate. Oh my god I do not care, she says.

Your phone runs out of battery so you give up for the day. Is that it? you ask. That depends what you mean by "it", replies Kate. It's a very good introduction to the HTTP protocol and a excellent placement of the protocol in the real world. If you understand everything that we just talked about then you'll be well-placed to work with the details of HTTP.

On the other hand, there's of course an entire career's-worth of extra stuff you can learn about. You could go up the stack and learn about how web browsers work. Security model is interesting.

You could go down the stack and look further into all the acronyms that we glossed over: TCP/IP, DNS, SSL/TLS, and so on.

Or you could go even deeper into the details of HTTP itself, perhaps learning more about some of the more commonly-used HTTP headers. Cross-Origin Resource Sharing (CORS) is interesting. You could look into the differences and similarities between HTTP 1.x and HTTP 2.

I really do recommend that you start by setting up Burp Suite or another HTTP proxy. It's a great way to get your hands dirty with some of the concepts we've discussed today. It's interesting to see what data your programs, websites, and smartphone apps are sending themselves. It's very fun to reverse engineer non-public APIs and send them your own requests. And it's fun to try to probe these apps for security vulnerabilities. Suppose you intercept an HTTP request from a note-taking app asking to load all the private notes for user number `12345`. What happens if you use your proxy to change that to `12346`? Hopefully the app's server does some authorization to see if you should be allowed to load that user's notes, but everyone makes misakes. The lower-quality and more rushed the app, the more likely an interesting vulnerability awaits for you to find *and tell the app's creators about*, like a responsible citizen. I'd recommend you give Wireshark a try too, although I think that it's rather more intimidating.

Since your phone is dead you ask Kate if she'll call you an Uber. She declines, pointing out that you always pull this crap and never pay her back. You don't know how public transport works in this city so you begin the lonely trudge home to the Outer Richmond.





In the most literal sense 








==============


### TODO POSSIBLE EXTRA TOPICS

* What is localhost?
* Where do cookies fit into this? (this is a good one)
* Difference between HTTP 1.x and HTTP 2
* What does it mean to listen on a port?

### TODO EXTENSIONS

* Use Postman
* Write your own HTTP client


Deleted section:

you can usually muddle through the tasks in front of you, using libraries and hand-waving to get the job done, after a fashion.


### Conventions and normality

Technology can be very flexible. The internet and the protocols that go into it are deliberately designed to be extensible for new use cases, and to allow different systems written in different languages by different companies to talk to each other.

Despite this flexibility, in practice most systems are built by piecing together the same components in the same ways. Servers can be anything, but they are usually racks of computers in purpose-built data centres. You can technically send HTTP requests via any communication medium that you like, but in practice you will always use TCP/IP. In general it's a good idea to follow these common paths unless you have a specific reason not to. Sometimes this is because the common paths are very good. Sometimes the common paths might be unfortunate accidents of history that make the world a bit worse, but it's still convenient to do whatever everyone else does.

Many choices are driven by flexible tradeoffs, not rigid technical requirements. The answer to "why can't I do X?" is often "well I suppose you could, but Y will probably work a little better." "Why can't I send my IP packets using carrier pidgeons?" "Well I suppose you could[LINK], but most people just send them over the internet."






A request target usually only needs to include the path (the bit after the `/`), and not the full URL (eg. `http://newspaper.com/articles/123`) because routing an HTTP request to the correct server on the internet is the job of the TCP/IP protocols, not the HTTP protocol.



API requests to create or update resources will also usually return the newly-created or updated resource to give the client full information about the operation that was just performed. We might expect a request to `POST /articles` to create a new article, and the HTTP response might look something like:

```json
TODO
```

If there was an error - say, the user didn't have the required permissions - then the response's status code should indicate a problem and the response body may give extra debug information (although it is not obliged to):

```json
{
    "success": false,
    "error": "Insufficient permissions"
}
```






To use an HTTP-based API, a client sends the server an HTTP request describing the action that it wants the server to perform. The request usually includes an authorization key that verifies the client's identity. The server performs the action, and sends back an HTTP response.


HTTP is like the rules for writing a formal letter. The letter-writing-protocol is a set of dictums for laying our a formal letter. You put your address in the top right, then the date, then "Dear So and So", then I don't know, who actually writes letters any more? The person receiving a letter also knows about the letter-writing protocol. They know that the lines in the top right are your address, and the date is your address. They know that if their name is Ms. Wrigglesworth and the letter is addressed to Mr. Clumpsford then something has probably gone wrong.





SImialrly, The letter writing protocol isn't a thing - it's an idea. It's written down in XYZ, and people use it to communicate with each other. Letter writing protocol isn't so important for humans - humans can easily fill in mistakes. But imagine you had two machines writing to each other. They need names, address, etc, to be in the exact right place. This is where HTTP is important.



The word "server" is often used to describe both the piece of software that makes services available, and the physical computer that this software runs on. 

The two-way radio protocol is a set of rules that allows humans to communicate over (almost always) a radio. In the same way, HTTP is a set of rules that allows applications to communicate over (almost always) an internet connection. I included those "almost always"-es because there's technically nothing stopping you using the two-way radio protocol to talk to your parents across the dinner table, and there's technically nothing stopping you sending HTTP messages to your friends via carrier pigeon. It's just that 




If you have a working understanding of the core of each of the above components then you'll understand most of HTTP. If you encounter any nuances or edge-cases ("how exactly do *CORS headers*[LINK] work again?") then you can research them on-the-fly. Very few people have, and no one should expect, an encyclopedic knowledge of any protocol. When you do need to sweat the details the Mozilla Developer Network (MDN) docs[LINK] are very thorough yet approachable.




A request target often refers directly to a tangible or abstract "object", such as an image, a user profile, or a collection of comments. However, it doesn't have to.


For example, my "About Me" page is at `robertheaton.com/about`. There's no such thing as an "about" object, although you could argue that this request refers to a page object that is called "about". Either way, this fine distinction isn't important. What is important is that, while a request target often clearly refers to an object, there are no technical rules or restrictions. A server is free to respond to requests for any request target with any type of information.






SS/KK

You ask if you can have a lunch break

You ask if you can have a bathroom break

You steeple your fingers in a manner that you hope can be described as “thoughtful”.

Your laptop is out of battery





You take notes on your phone because you aren't sure where your computer is. You 
Your phone notes are garbled and make no sense but you don't want to look like you've lost interest
You ostentatiously start making flashcards


You want to look clever
You want to cover up looking like a dummy

You cancel your singing lessons





KK wants to help





Because HTTP is a client-server protocol, HTTP conversations only happens when a client (like your phone or computer) initiates one. For example, when you click on a profile in the Facebook app, the app sends requests asking for the data it needs to populate the profile page.

[TODO-PIC] 

Since your phone doesn't act as an HTTP server, it doesn't listen for incoming requests in the same way that Facebook servers do. This means that Facebook can't contact your phone until your phone contacts it. Even though Facebook is famous for smuggling out personal data from your devices, they can only do this by writing code in their app that pro-actively sends it to them. The Facebook app or website has to grab the data that it wants, and then siphon it off by sending an HTTP request to the Facebook servers. The Facebook servers can't reach out and ask for the data; they have to wait until it is sent to them. Facebook gets your data either way, but from a technical point of view the distinction is important.
