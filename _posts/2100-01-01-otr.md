---
layout: post
title: "Off-The-Record Messaging explained (very clearly?)"
tags: [Security]
og_image: TODO
---
<style>
    img {
        image-rendering: -webkit-optimize-contrast;
    }
</style>

## 0. Introduction

Suppose that you want to have a clandestine conversation with a friend in a local park. If you secure any nearby shrubberies then you can be confident that no one will be able to eavesdrop on you. And unless you're being secretly recorded or filmed, which in most situations is unlikely, you can be similarly confident that no one will be able to make a permanent, verifiable record of the conversation. This means that you and your friend can have a private, off-the-record exchange of views. Local parks - and the the physical world in general - have reasonably strong and intuitive privacy settings.

However, in the cyberworld these settings get more complex. On the internet no one can hide in the bushes and overhear your email or instant messages, but they can glimpse your screen over your shoulder or intercept your traffic as it travels over a computer network. There's no record of most real-world conversations apart from the other person's word, but electronic messages create a papertrail, which is both useful and incriminating. These differences between the fleshy and computerized worlds don't necessarily increase or decrease net privacy, but they do change the threats and vectors that security- and privacy-sensitive actors within them need to consider.

In the physical world, a brief lapse in communication security usually has limited consequences. It might allow a snooper to overhear a single conversation, or a disgruntled confederate to loosely retell, without proof, the details of a past discussion. However, in the cyberworld, brief lapses can be catastrophic and the consequences can be even worse than simply exposing confidential information.

Suppose that, as happens all too often, an attacker steals a dump of electronic messages and the secret key needed to decrypt them. The attacker will be able to open and read the previously-secret messages, and, to make matters worse, they may also be able to exploit the messages' encryption protocol to prove to the world that the stolen messages are real, preventing the victims from claiming that they are fakes that should be ignored. Furthermore, the attacker may even be able to use the compromised secret key to decrypt years worth of historical messages that were also encrypted using that same key.

Fortunately, secure communication processes can be designed to address these problems. There's much more to cryptography than just encrypting and decrypting messages, and the extra details matter. Where do encryption keys come from? How do the sender and receiver decide on their keys? How often do they need to generate new keys? What happens to keys once they've been used? How do the sender and receiver authenticate each other's identities? The collection of rules that answer these questions and that a sender and receiver use to exchange private messages is called a *cryptographic protocol*.

*Off-The-Record Messaging* (OTR) is a cryptographic protocol that aims to replicate the privacy properties of casual, in-person conversation. The primary goal of the OTR protocol is that no one can overhear an OTR exchange. On top of this, it guarantees that if someone somehow does gain access to an OTR message then they can't prove that it is real, and they certainly can't decrypt and read reams of historical messages too. Any fool with decades of experience in high-grade cryptography can design a communication protocol that is secure when things go right. OTR focuses on what happens when things go wrong.

OTR does all this using standard, off-the-shelf ciphers and algorithms. Its (relatively) simple genius is in how it arranges old tools in novel patterns. In order to understand the innovations of OTR we don't need to pore over pages of mathematical proofs. Instead we have to think extremely precisely about the system-level ways in which familiar primitives are chained together. OTR inspired the first version of the Signal Protocol, the eponymous protocol used by the secure messaging app Signal, and learning how OTR works will invigorate your own cryptographic imagination too.

### 0.1 About this long blog post/short book

This long blog post/short book is based on [Borisov, Goldberg and Brewer 2004][otr-paper], the paper that first introduced OTR, and it assumes very little prior knowledge of cryptography. It can't hurt to be familiar with concepts like public and private keys and cryptographic signatures, but don't worry if you aren't, we'll cover what you need to know. Whereas Borisov, Goldberg and Brewer had but 7 pages in an academic journal, we have as much space as we like in which to clarify details, ponder the ways in which OTR intersects with the 2020 US Presidential Election, and see how dead bodies in a series of convolutedly-structured rooms illustrate how a protocol can simultaneously achieve both authentication and deniability.

----

## 1. Why do we need Off-The-Record Messaging?

Let's consider Alice and Bob. Alice and Bob want to talk to each other online, but, as always, their privacy is under threat from the evil Eve. Eve is trying to intercept their traffic; compromise their computers; and generally try to find out what they are saying and manipulate the contents of their messages. Alice and Bob want to stop her and talk to each other privately and reliably.

### 1.1 PGP

One way in which Alice and Bob can prevent Eve from reading or spoofing their communication is by using a standard cryptographic protocol like PGP (Pretty Good Privacy) that provides both *encryption* (preventing Eve from reading their messages) and *authentication* (preventing Eve from spoofing fake messages). PGP is good at facilitating secret, verifiable communication. However, it comes with unexpected and often undesirable side-effects. Before we examine the weaknesses of PGP, let's see how it works when everything goes according to plan.

PGP can be used to both *encrypt* and *sign* a message (more on which in a moment). PGP performs encryption using an *asymmetric cipher*, which means that encryption and decryption are performed using different keys. In order to be able to receive PGP messages, a person must generate a pair of linked keys, called a keypair. One of these keys is their private key, which they must keep secret and safe at all costs. The other is their public key, which they can safely publish and make available to anyone who wants it, even their enemies. 

<img src="/images/otr/otr-0.png" />

Messages encrypted with a person's public key can only be decrypted with their private key, and vice-versa. The fact that a message encrypted with a public key can only be decrypted using the corresponding private key is useful when sending secret messages.

<img src="/images/otr/otr-1.png" />

Suppose that Alice wants to use PGP to send an encrypted message to Bob. She retrieves Bob's public key from wherever he has published it and uses it to encrypt her message. She sends the resulting ciphertext to Bob. Bob uses his private key to decrypt and read it. Since the message can only be decrypted using Bob's private key, which only Bob knows, Alice and Bob can be confident that Eve can't read their message.

<img src="/images/otr/otr-3.png" />

However, even though Bob is the only person who can decrypt Alice's message, Bob has no strong proof that the message really was written by Alice. As far as he's concerned it could have been written and encrypted by Eve, or Eve could have intercepted and manipulated a real message from Alice. To prove to Bob that the message really was written by her, Alice uses PGP to *cryptographically sign* her message before sending it.

We've seen how messages encrypted using a public key can only be decrypted using the corresponding private key. In order to sign her message, Alice uses her own public and private keypair and exploits the equivalent fact that a message encrypted with a *private* key can only be decrypted using the corresponding *public* key.

<img src="/images/otr/otr-2.png" />

To generate a cryptographic signature, Alice starts by passing her message through a *hash function*, which is an algorithm that produces a random-seeming but consistent and fixed-length output for a given input. She then encrypts the output of the hash function with her private key. The result of this encryption is a *cryptographic signature* that proves that the corresponding message was indeed written by Alice (we'll see why in a second). Alice appends the signature to her encrypted message, and sends them both to Bob.

<img src="/images/otr/otr-4.png" />

[TODO-above-iamge-updated-recopy]
[TODO-hash-or-encrypt-first?]

When Bob receives Alice's message and signature, he uses the signature to prove to himself that the message was written by Alice and has not been tampered with. He does this by performing a similar but different process to the one that Alice used to generate the signature. He starts by calculating the hash of Alice's message, just as Alice did, but he then uses Alice's *public* key to *decrypt* Alice's signature. If the resulting plaintext matches the hash of Alice's message then Bob can be confident that the signature was generated using Alice's private key, and that the contents of the message haven't changed. Since Alice is the only one who knows her private key, she is the only one who could have generated the signature. Bob can therefore infer that the message really was written and signed by Alice.

Messages are hashed before they are signed because the output of a hash function is always of the same, fixed length. This means that signatures generated by encrypting these hashes are also of the same fixed length. If messages were not hashed before they were signed then a long message would produce a long signature that would require unnecessary bandwidth to transmit, and a tiny message would produce a tiny signature that could be easy to forge.

<img src="/images/otr/otr-5a.png" />
<img src="/images/otr/otr-5b.png" />

[TODO-above-iamge-updated-recopy]

Encryption and signatures are how PGP gives Alice and Bob privacy and authentication. If Eve intercepts an encrypted message then she won't be able to decrypt it because she doesn't know Bob's private key. And if Eve tries to spoof a fake message to Bob from Alice then she won't be able to produce a valid signature, because she doesn't know Alice's private key either. When Bob tries to verify a signature forged by Eve, he will discover that something is afoot. If everything goes perfectly, PGP works perfectly.

However, in the real world, we also have to plan for the times when everything does not go perfectly. PGP relies heavily on private keys staying private. If a user's private key is stolen then the security properties previously underpinned by it are blown apart. If Eve steals a copy of Bob's private key and intercepts Alice's message on its way to Bob, Eve can decrypt and read the message.

<img src="/images/otr/otr-6.png" />

If Eve also steals a copy of Alice's private key then she can use it to sign fake messages that Eve herself has written, making it look like these messages came from Alice.

All encryption is to some degree underpinned by the assumption that secret keys stay secret, and so we should expect any protocol to be severely damaged if this assumption is broken. But with PGP the fallout is worse than just leaking confidential messages. We know already that if Eve steals a private key then she can use it to decrypt any future PGP messages that she intercepts and that were encrypted using the corresponding public key. But suppose that Eve has been listening to Alice and Bob for months or years, patiently storing all of their encrypted traffic. Previously she had no way to read any of this traffic, but with access to Bob's private key she can now read all of the historical traffic that she has stored and that was also encrypted using Bob's public key. This gives her access to stacks of old, previously-secret messages.

But it gets *even* worse. Alice cryptographically signed all her PGP messages so that Bob could be confident that they were authentic. However, whilst generating a signature requires Alice's private key, *verifying* her signature only requires her public key, which is typically freely available. This means that Eve can use Alice's signatures to verify the validity of messages sent by Alice and stolen from Bob just as well as Bob can. If Eve gets access to Alice and Bob's signed messages, she also gets access to a cryptographically verifiable transcript of their communications. This prevents Alice and Bob from credibly denying that the stolen messages are real. In a few sections' time we'll look at some real-world examples of how cryptographic signatures like this can cause serious problems for the victims of hacks.

PGP does give secrecy and authentication. But it's brittle, and it isn't robust to private key compromise. We'd ideally like an encryption protocol that better mitigates the consequences of a hack. In particular we'd like two extra properties. First, we'd like *deniability*, which is the ability for a user to credibly deny that they ever sent hacked messages. And second, we'd like *forward secrecy*, which is the property that if an attacker compromises a user's private key then they are still unable to read past traffic that was encrypted using that keypair.

Let's examine these properties in detail and see why they are so desirable. Then we'll look at how Off-The-Record-Messaging works and how it achieves them.

### 1.2 Deniability

*Deniability* is the ability for a person to credibly deny that they knew or did something. Statements from in-person conversations are usually easily deniable. If you claim that I told you that I planned to rob a bank then I can credibly retort that I didn't and you can't prove otherwise. Email conversations can be deniable too (although see below for a look into why in practice they often aren't). Suppose that you forward to a journalist the text of an email in which I appear to describe my plans to rob a hundred banks. I can credibly claim that you edited the email or forged it from scratch. Once again, you can't prove that I'm lying. The public or the police or the judge might still believe your claims over mine, but nothing is mathematically provable and we're down in the murky world of human judgement.

By contrast, we've seen that PGP-signed messages are not deniable. If Alice signs a message and sends it to Bob then Bob can use the signature to validate that the message is authentic. However, anyone else who comes into possession of the message can also validate the signature and prove that Alice sent the message in exactly the same way that Bob did. Alice is therefore permanently on the record as having sent these messages. If Eve hacks Bob's messages, or if Alice and Bob fall out and Bob forwards their past communication to her enemies, Alice cannot plausibly deny having sent the messages in the same way as she could if she had never signed them. If you forward to a journalist an email that I cryptographically signed in which I describe my plans to rob a thousand banks, I will be in a pickle.

On the face of it, deniability is in tension with authentication. Authentication requires that Bob can prove to himself that a message was sent by Alice. By contrast, deniability requires that Alice can deny having ever sent that same message. As we'll see, one of the most interesting innovations of OTR is how it achieves these seemingly contradictory goals simultaneously.

Of course, technically anything can be denied, even cryptographic signatures. I can always claim that someone stole my computer and guessed my password, or infected my computer with malware, or stole my private keys while I was letting them use my computer to look up the football scores. These claims are not impossible, but they are unlikely and tricky to argue. Deniability is a sliding scale of plausibility, and so OTR goes to great lengths to make denials more believable and therefore more plausible. "A dog ate my homework" is a much more credible excuse if you ostentatiously purchased twenty ferocious dogs the day before.

It's still perfectly reasonable to believe that a deniable message is nonetheless authentic. We all assess and believe hundreds of claims every day using vague balances of probability, without mathematical proof. Screenshots of a WhatsApp conversation might reasonably be enough to convince the authorities of my plans to rob ten thousand banks, even without cryptographic signatures.

But if signatures are available then that does make everything much easier.

#### 1.2.1 DKIM, the Podesta Emails, and the Hunter Biden laptop

For message senders, deniability is almost always a desirable property. There's rarely any advantage to having everything you say or write go into an indelible record that might come back to haunt you. This is not the same thing as saying that deniability is an objectively good thing that makes the world strictly better. Just take the intertwined stories of the DKIM email verification protocol; US Democratic Party operative John Podesta; and the laptop of Hunter Biden, son of President Joe Biden.

In order to understand the politics, we first need to discuss the protocols. In the old days, when an email provider received an email claiming to be from `rob@robmail.com`, there was no way for the provider to verify that the email really was sent by `rob@robmail.com`. The provider therefore typically crossed their fingers, hoped that the email was legitimate, and accepted it. Spammers abused this trust to bombard email inboxes with forged emails. The DKIM protocol was created in 2004 to allow email providers to verify that the emails they receive are legitimate, and the protocol is still used to this day.

DKIM uses many of the same techniques as PGP. In order to use DKIM (which stands for *Domain Keys Identified Mail*), email providers generate a signing keypair and publish their public key to the world (via a DNS TXT record, although the exact mechanism is not important to us here). When a user sends an email, their email provider generates a signature for the message using the provider's private signing key. The provider inserts this signature into the outgoing message as an email header.

<img src="/images/otr/otr-7.png" />
[TODO-recopy-pic]

When the receiver's email provider receives the message, it looks up the sending provider's public key and uses it to check the DKIM signature against the email's contents, in exactly the same way as a recipient would check a PGP message signature against a PGP message's contents. If the signature is valid, the receiving provider accepts the message. If it isn't, the receiving provider assumes that the message is forged and rejects it. Since spammers don't have access to mail providers' signing keys, they can't generate valid signatures. This means that they can't generate fake emails that pass DKIM verification, making DKIM very good at detecting and preventing email forgery.

But as we've seen, where there's a cryptographic signature, there might also be a problem with deniability. As well as providing authentication, a DKIM signature also provides permanent, undeniable proof that the signed message is authentic, in much the same way as a PGP signature does. DKIM signatures are part of the contents of an email, so they are saved in the recipient's inbox. If a hacker steals all the emails from an inbox, they can validate the DKIM signatures themselves using the sending provider's public DKIM key, in the same way as they would validate a stolen PGP signature. The attacker already knows that the emails are legitimate, since they stole them with their own two hands. However, DKIM signatures allow them to prove this fact to a sceptical third-party as well, obliterating the emails' deniability. Email providers change, or *rotate*, their DKIM keys regularly, which means that the public key currently in their DNS record may be different to the key used to sign the message. Fortunately for the attacker, historical DKIM public keys for many large mail providers can easily be found on the internet.

Matthew Green, a professor at Johns Hopkins University, [points out](https://blog.cryptographyengineering.com/2020/11/16/ok-google-please-publish-your-dkim-secret-keys/) that making emails non-repudiatable in this way is not one of the goals of the DKIM protocol. Rather, it's an odd side-effect that wasn't contemplated when DKIM was originally designed and deployed. Green further argues that DKIM signatures make email hacking a much more lucrative pursuit. It's hard for a journalist or a foreign government to trust a stolen dump of unsigned emails sent to them by an associate of an associate of an associate, since any of the people in this long chain of associates could have faked or embellished the emails' contents. However, if the emails contain cryptographic DKIM signatures generated by trustworthy third parties (such as a reputable email provider), then the emails are provably real, no matter how questionable the character who gave them to you. Cryptographic signatures don't decay with social distance or sordidness. Data thieves are able to piggy-back off of Gmail's (or any other DKIM signer's) credibility, making stolen, signed emails a verifiable and therefore more valuable commodity. 

This causes problems in the real world. In March 2016, Wikileaks published [a dump of emails](https://wikileaks.org/podesta-emails/) hacked from the Gmail account of John Podesta, a US Democratic Party operative. Alongside each email Wikileaks published [the corresponding DKIM signature](https://wikileaks.org/podesta-emails/emailid/10667), generated by Gmail or whichever provider sent the email. This allowed independent verification of the messages, which prevented Podesta from claiming that the emails were nonsense fabricated by liars.

You may think that the Podesta hack was, in itself, a good thing for democracy, or a terrible thing for a private citizen. You may believe that the long-term verifiability of DKIM signatures is a societal virtue that increases transparency, or a failing that incentivizes email hacking. But whatever your opinions, you'd have to agree that John Podesta definitely wishes that his emails didn't have long-lived proofs of their provenance, and that most individual email users would like their own messages to be sent deniably and off-the-record.

Matthew Green has a counter-intuitive but elegant solution to this problem. Google already regularly rotate their DKIM keys. They do this as a best-practice precaution, in case the keys have been compromised without Google realizing. Green proposes that, once Google (and other mail providers) have rotated a DKIM keypair and taken it out of service, they should *publicly publish* the keypair's *private key*.

Publishing an old private key would mean that anyone could use it to spoof a valid-looking DKIM signature for an email. If the keypair were still in use, this would make Google's DKIM signatures useless. However, since the keypair has been retired, revealing the private key does not jeopardize the effectiveness of DKIM in any way. The purpose of DKIM is to allow *email recipients* to verify the authenticity of a message *at the time they receive that message*. Once the recipient has verified and accepted the message, they need never verify it again. The signature has no further use unless someone - such as an attacker - wants to later prove the provenance of the email.

Since DKIM only requires signatures to be valid and verifiable when the email is received, it doesn't matter if an attacker can spoof a signature for an old email. In fact it's desirable, because it renders worthless all DKIM signatures that were legitimately generated by the email provider using the now-public private key. Suppose that an attacker has stolen an old email dump containing many emails sent by Gmail, complete with DKIM signatures generated using Gmail's now-public private key. Previously, only Google could have generated these signatures, and so the signatures proved that the emails were genuine. However, since the old private key is now public, anyone can use it to generate valid signatures themselves. All a forger has to do is take the now-public private DKIM key of the email provider, plus the contents of the email that they want to sign. They use the key to sign the email, in much the same way as they would sign a PGP message. Finally, they add the resulting signature as an email header, just as the email provider would.

This means that unless an attacker with a stolen batch of signed emails can prove that their signatures were generated while the key was still private (which in general they won't be able to), the signatures don't prove anything about anything to a sceptical third party. This applies even if the attacker really did steal the emails and is engaging in only a single layer of simple malfeasance.

But how much difference does any of this really make? Wouldn't everyone have believed the Podesta Emails anyway, without the signatures? Possibly. But consider a more recent example in which I think that DKIM signatures could have changed the course of history, had they been available.

During the 2020 US election campaign, Republican operatives claimed to have gained access to a laptop belonging to Hunter Biden, son of the then-Democratic candidate and now-President Joe Biden. The Republicans claimed that this laptop contained gobs of explosive emails and information about the Bidens that would shock the public.

The story of how the Republicans allegedly got hold of this laptop is somewhat fantastical, winding by way of a computer repair shop in a small town in Delaware. However, fantastical stories are sometimes true, and this is exactly the type of situation in which cryptographic signatures could play a big role in establishing credibility. It doesn't matter how wild the story is if the signatures validate. Indeed, in an effort to prove the laptop's provenance, Republicans released a single email, with [a single DKIM signature](https://github.com/robertdavidgraham/hunter-dkim), that they claimed came from the laptop.

<img src="/images/otr/otr-8.png" />
[TODO-havescreenshotofactualsigandhighlightit-andmaybeincludebodywhynot]

The DKIM signature of this email is valid, and when combined with the email body (not shown above) it does indeed prove that `v.pozharskyi.ukraine@gmail.com` sent an email to `hbiden@rosemontseneca.com` about meeting the recipient's father, sometime between 2012 and 2015. However, it also raises a lot of questions, and provides a perfect example of the sliding-scale nature of deniability. The single email doesn't prove anything more than what it says. It doesn't prove who `v.pozharskyi.ukraine@gmail.com` is (although other evidence might). It doesn't prove that there are thousands more like it, and it doesn't prove that the email came from the alleged laptop.

The email is cryptographically verifiable, but without further evidence the associated story is still plausibly deniable. I suspect that if a full dump of emails and signatures had been released, a la Wikileaks and Podesta, then their combined weight could have been enough to overcome many circumstantial denials. If the Republicans were also sitting on a stack of additional, also-cryptographically-verifiable material then it's hard to understand why they chose to release only this single, not-particularly-incendiary example. Maybe the 2020 election could have gone a different way. Cryptographic deniability is important.

----

You can't deny that we've studied deniability in depth, so now let's look at the other new property that Off-The-Record-Messaging provides: forward secrecy.

## 1.3 Forward Secrecy

Alice and Bob are once again exchanging encrypted messages over a network connection. Eve is intercepting their traffic, but since their messages are encrypted she can't read them. Nonetheless, Eve decides to store Alice and Bob's encrypted traffic, just in case she can make use of it in the future.

A year later, Eve compromises one or both of Alice and Bob's private keys. For many encryption protocols, Eve would be able to go back to her archives and use her newly stolen keys to decrypt all the encrypted messages that she has stored over the years. However, if Alice and Bob were using an encryption protocol with the remarkable property of *forward secrecy* then Eve would not be able to decrypt their stored historical traffic, even with access to their private keys.

Cryptography is a very precise field so I'll start by quoting a technical definition of forward secrecy to show that I'm also a very precise person. We don't need to know the details of forward secrecy, but a quick synopsis will be important to understanding OTR:

> Forward Secrecy is a feature of specific key agreement protocols that gives assurances your session keys will not be compromised even if the private key of the server is compromised. *([Wikipedia](https://en.wikipedia.org/wiki/Forward_secrecy))*

Before we wade into what this means, we'll need a few new cryptographic definitions. An encryption algorithm, or *cipher*, can be either symmetric or asymmetric. A symmetric cipher is one in which the same key is used to encrypt and decrypt the message.

<img src="/images/otr/otr-9.png" />

By contrast, an asymmetric cipher is one in which different keys (typically one public and one private) are used for encryption and decryption, as in PGP.

<img src="/images/otr/otr-10.png" />

We've seen already that encrypting a message directly using the recipient's public key does not give forward secrecy. This is what PGP does, and we've seen that if an attacker compromises a PGP recipient's private key then the attacker can read all of their PGP-encrypted messages.

Encryption protocols that provide forward secrecy typically require Alice and Bob to use a symmetric cipher. Since a symmetric cipher uses the same key to encrypt and decrypt a message, Alice and Bob need to somehow agree on a shared, symmetric key that they both know the value of. As we'll soon see, in order to achieve forward secrecy they must use each symmetric key only briefly before forgetting it and agreeing on a new one. These temporary symmetric keys are often called *ephemeral* or *session keys*.

<img src="/images/otr/otr-11.png" />
[TODO-recopy]

Alice and Bob agree on each symmetric key via a process known as a *key-exchange*, which we'll look at in more detail shortly. Once Alice and Bob have agreed on a symmetric key, they use it to encrypt and decrypt a small number of messages (exactly 1, in the most cautious implementations). Finally, once they've finished with a key, Alice and Bob "forget" it, wiping it from their RAM, disk, and anywhere else from which an attacker might be able to recover it.

Alice and Bob need to agree on each symmetric key in a manner that prevents Eve from working out the value of the key, even if she watches all of their key-exchange traffic. However, agreeing on a secure session key over an insecure network is difficult. Alice can't simply choose a session key and send it in plaintext to Bob, because Eve could intercept the message containing the key and use it to decrypt any future messages encrypted using it.

Fortunately, Alice and Bob still have their public and private keypairs, even if they don't want to use them for encrypting their actual messages. What if Alice encrypts the session key with Bob's public key first? This is the approach taken by some key-exchange protocols, and it is certainly an improvement. It will keep the session key secret, but it still won't provide forward secrecy. In this type of key-exchange, Alice chooses a random shared, secret, session key and then uses Bob's public key to encrypt it. Alice then sends the encrypted session key to Bob, who uses his private key to decrypt it. Since Eve does not have access to Bob's private key, she cannot decrypt and read the session key. Alice and Bob have therefore successfully agreed on a secret session key, and can use it to symmetrically encrypt and decrypt messages.

<img src="/images/otr/otr-12.png" />

This approach keeps session keys secret so long as Bob's private key stays secret, but it does not provide forward secrecy. Suppose that Eve stores all of Alice and Bob's encrypted traffic and then later compromises Bob's private key. She can use this key to decrypt the encrypted session keys, and then use those session keys to decrypt the actual messages that Alice sent to Bob.

To give forward secrecy - and this is the very clever part - Alice and Bob must agree on each session key using a protocol that prevents Eve from working out its value, even if she also compromises their long-lived private keys. Alice and Bob still use their private keys during these key-exchanges, but only in order to prove their identities to each other.

The original OTR paper achieves this goal using a key-exchange protocol called *Diffie-Hellman* (although many other similar protocols would also work), which we'll look at more later. If Alice and Bob agree their session keys using a carefully designed key-exchange protocol like Diffie-Hellman and fully forget them once they're no longer needed, then there is no way for anyone to work out what the values of those keys were; even the attacker; and even Alice and Bob. There is therefore no way, in any circumstances, for anyone to decrypt traffic that was encrypted using one of these forgotten keys. Forward secrecy, and with it one of the primary goals of OTR, is achieved.

Forward secrecy is often called "perfect forward secrecy", including by many very eminent cryptographers. Other, similarly eminent cryptographers object to the word "perfect" because (my paraphrasing) it promises too much. Nothing is perfect, and even when a forward secrecy protocol is correctly implemented, its guarantees are not perfect until the sender and receiver have finished forgetting a session key. Between the times at which a message is encrypted and the key is forgotten lies a window of opportunity for a sufficiently advanced attacker to steal the session key from the participants' RAM or anywhere else they might have stored it. In practice the guarantees of forward secrecy are still strong and remarkable. But when discussing Off-The-Record Messaging, a protocol whose entire reason for existence is to be robust to failures, these are exactly the kinds of edge-cases we should contemplate. We'll talk about this more in later sections.

----

It's clear why encryption and authentication are desirable properties for a secure messaging protocol. Encryption prevents attackers from reading Alice and Bob's messages, while authentication allows Alice and Bob to be confident that they are each talking to the right person and that their messages have not been tampered with.

We've also seen why deniability and forward secrecy are important. Deniability allows participants to credibly claim not to have sent messages in the event of a compromise. This replicates the privacy properties of real-life conversation and makes the fallout of a data theft less harmful. Forward secrecy, the other property that we discussed, prevents attackers from reading historical encrypted messages, even if they compromise the long-lived private keys used at the base of the encryption process.

The goal of OTR is to achieve all of these properties simultaneously, and now we're ready to see how it does this. We'll start by looking at the broad mechanics of how an OTR exchange works, then we'll delve deeper into its subtler design decisions.

## 2. How does Off-The-Record Messaging work?

At a high level, an OTR exchange looks similar to that of many other encryption protocols.

In order to exchange an OTR message, the sender and recipient must:

<ol>
  <li><a href="#21-agreeing-on-an-encryption-key">Agree on a secret encryption session key</a></li>
  <li><a href="#22-verifying-identities">Verify each other's identities</a></li>
</ol>

Then the sender must:

<ol start="3">
  <li><a href="#23-encrypting-a-message">Encrypt the message</a></li>
  <li><a href="#24-signing-a-message">Sign the message</a></li>
  <li><a href="#25-sending-the-message">Send the message</a></li>
</ol>

And the recipient must:

<ol start="6">
  <li><a href="#26-decrypting-and-verifying-the-message">Decrypt the message and verify its signature</a></li>
</ol>

Then the sender must:

<ol start="7">
  <li><a href="#27-an-unexpected-twist-publishing-the-signing-key">Publish the previously-secret encryption key (yes, this is weird)</a></li>
</ol>

Finally, the sender and recipient must both:

<ol start="8">
  <li><a href="#28-forgetting-the-encryption-key">Forget the encryption session key in order to preserve forward secrecy</a></li>
</ol>

This sounds straightforward enough, but OTR's insatiable hunger for deniability and forward secrecy mean that there's a lot of nuance in those little bullet points. There are also a few extra steps that I've left out for now because they won't make much sense until we get there.

Let's start at the top.

### 2.1 [Agreeing on an encryption key](#21-agreeing-on-an-encryption-key)

Alice and Bob start by agreeing on a shared, symmetric, ephemeral session key. They will later use this key to encrypt a message using a symmetric cipher. In order to preserve forward secrecy, once they have finished exchanging a message, they will forget the session key that they used to send it and re-agree on a new one. The exact cadence at which they agree new session keys is discussed in detail in [the original OTR paper][otr-paper], but for our purposes we can assume that session keys are rotated roughly every message.

<img src="/images/otr/otr-13.png" />

Alice and Bob securely agree on each shared secret key, without leaking it to Eve, using a process called Diffie-Hellman key exchange. The guts of Diffie-Hellman are complicated and not necessary in order to understand OTR, but a broad understanding is nonetheless instructive.

#### 2.1.1 Diffie-Hellman key exchange

Roughly speaking, to begin a Diffie-Hellman key exchange Alice and Bob each separately choose a random secret number. They each send the other a specially-chosen number that is derived from, but isn't, their own random secret number. Now each knows their own random secret number, as well as the derived number that the other sent them. Thanks to the careful construction of the Diffie-Hellman protocol, each can use this information to derive the same final, secret key, which they can use as their shared session key.

[TODO-pic]

Even if Eve snoops on their communication and sees both of the derived numbers that they exchanged, she is unable to compute the final, secret key, because she doesn't know either of their initial random secrets.

<img src="/images/otr/otr-14.png" />

Wikipedia has [a good analogy](https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange#General_overview) that uses paint instead of numbers, but if you don't get it then don't worry, the details aren't important to us. What matters is that Diffie-Hellman allows Alice and Bob to agree on a shared symmetric key in a way that gives them forward secrecy and prevents Eve from discovering the key.

### 2.2 [Verifying identities](#22-verifying-identities)

We've described how Alice and Bob agree on a shared secret session key that they can use for symmetric encryption. However, we haven't yet considered how they can verify the identity of the person they've agreed that session key with. At the moment Eve could intercept and block all of Alice and/or Bob's traffic, and then spoof a separate Diffie-Hellman key-exchange with each person herself! Alice and Bob would each have agreed a shared secret, but with Eve instead of each other. Eve would be able to read their messages and spoof replies to them, and Alice and Bob would have no way of knowing what had happened.

<img src="/images/otr/otr-15.png" />
[TODO-makediagramandtextmatch]

Even though OTR doesn't use public/private keypairs (often also referred to as *public key cryptography*) to directly encrypt its messages, it does use it for identity verification. In their original paper ([Borisov, Goldberg and Brewer 2004][otr-paper]), the researchers who devised OTR recommended an elegant way in which Alice and Bob could use public/private keypairs to verify each other's identities when performing a Diffie-Hellman key exchange.

However, a few years later another paper ([Alexander and Goldberg 2007](https://www.cypherpunks.ca/~iang/pubs/impauth.pdf)) was published that described a weakness in this approach. The second paper proposed a broadly similar but more complex alternative. Here we'll discuss the original, simpler, if slightly flawed procedure, since it's still a good illustration of the general principle of authenticating a key-exchange.

In the original OTR formulation, in order to prove their identities to each other, Alice and Bob each signed the intermediate Diffie-Hellman values that they sent to the other person. Then, when they received a signed intermediate value, they verified the signature against the other person's public key. If the signature didn't check out, they aborted the conversation. If it did, they could be sure that the secret value corresponding to this intermediate value was known only to the person who signed the intermediate value. They could proceed to combine the intermediate value with their own secret value in order to produce the shared symmetric secret key. The intention of this process was to allow Alice and Bob to each be sure that they were agreeing a shared secret with the right person.

<img src="/images/otr/otr-16.png" />

Eve could still intercept and try to tamper with Alice and Bob's traffic, but she wasn't be able to get them to accept an intermediate Diffie-Hellman value derived from her own random secret. She therefore wasn't be able to trick them into negotiating an encrypted connection with her instead of each other.

However, Alexander and Goldberg 2007 showed that Eve could still manipulate this type of key exchange in other ways. Eve could have Alice and Bob negotiate a connection with each other, but trick Bob into believing that he was talking to Eve when he was really talking to Alice. Eve wasn't able to actually read any messages, but this was nonetheless an unacceptable level of shennanigans to allow in an otherwise robust protocol. OTR therefore now uses the more intricate approach outlined in Alexander and Goldberg 2007, in which Alice and Bob use vanilla Diffie-Hellman to set up an encrypted but unauthenticated connection, and then authenticate each other's identities inside that channel.

Astute readers may be surprised that OTR ever used signatures to verify participants' identities. Haven't we been saying that public/private key cryptographic signatures are the enemy of deniability? We've seen in some detail how the message signatures used by PGP are effective for authentication, but bad for deniability.

However, Alice and Bob never actually use their private keys to sign their messages. All they sign are their intermediate key exchange values, and all that these signatures prove is that Alice and Bob exchanged two random-looking values. They don't prove that Alice and Bob exchanged any specific messages, meaning that their conversations remain deniable. This said, Alice and Bob do still somehow need to sign their messages in order to authenticate their contents and ensure that they aren't tampered with. We'll soon see how they do this while preserving deniability.

Alice and Bob have now agreed on a shared, symmetric, ephemeral session key and verified each other's identities. Next, Alice needs to encrypt her message.

### 2.3 [Encrypting a message](#23-encrypting-a-message)

At this point, Alice and Bob have agreed on a secret, symmetric session key. We now need to specify a symmetric encryption algorithm (or *cipher*) that they can combine with their key in order to encrypt and decrypt their messages. Many such encryption algorithms exist already, and OTR can use a pre-existing algorithm without inventing its own.

For counter-intuitive reasons that we will discuss shortly, OTR requires a *malleable* cipher. A malleable cipher is one for which it is deliberately possible for an attacker to manipulate a ciphertext so that it decrypts to an alternative plaintext chosen by the attacker. For example, the attacker can tweak an encrypted message that decrypts to "Release the prisoner" into one that decrypts to "Execute the prisoner". This message will be decryptable using the symmetric key agreed by the sender and receiver, which means that the receiver will be inclined to believe that it is legitimate.

In order to exploit a malleable cipher the attacker needs to be able to guess the plaintext that the original ciphertext decrypts to, and the alternative plaintext must be the same length as the original. However, the attacker can do their fiddling without knowing the keys that were used to encrypt or decrypt the original ciphertext. 

[TODO-pic]

Malleability is usually not a desirable property, but we'll see later why it is oddly useful for OTR. OTR uses a malleable cipher called a *stream cipher with AES in counter mode*, but the details of how this particular cipher works aren't important to us here.

Alice and Bob have now agreed on a key, verified each other's identities, and have an encryption cipher that they can use. They haven't used their private keys to sign anything important, which means that there's nothing cryptographically tying them back to their message in the event of a compromise. They are therefore ready to use their key and cipher to encrypt, exchange, and decrypt a message.

However, we haven't yet given Alice and Bob a way to detect whether their encrypted messages are tampered with in transit. The fact that a stream cipher is malleable, and therefore particularly easy to manipulate, makes this check especially important for OTR. Let's see how OTR authenticates the contents of its messages without jeopardizing Alice and Bob's ability to deny that they sent them.

### 2.4 [Signing a message to authenticate it](#24-signing-a-message)

In order to authenticate the contents of a message, the sender needs to be able to sign it and the receiver needs to be able to validate this signature. However, Alice and Bob don't want to sign their messages directly using their private signing keys (which is what PGP does), because this would severely impair the messages' deniability.

OTR therefore has to prove the integrity of its messages using a different type of cryptographic signature. We know already that OTR uses a symmetric encryption algorithm to encrypt its messages while preserving forward secrecy. OTR also uses a symmetric signing algorithm to authenticate its messages while preserving deniability. Whereas an asymmetric signing algorithm (such as the one used by PGP) uses one key in a keypair to create a signature and the other key to verify it, a symmetric one uses the same key to both create and verify. We'll see why this is important shortly.

<img src="/images/otr/otr-17.png" />

OTR uses a symmetric signing algorithm called HMAC, which stands for Hash-Based Message Authentication Code. In order to generate an HMAC signature for a message, the signer passes both their message and their symmetric, shared, secret signing key into the HMAC algorithm (we don't need to know any internal details about HMAC, but we'll talk about where this secret key comes from in a few paragraphs' time). The algorithm returns an HMAC signature, and the signer sends the message and HMAC signature to the recipient.

In order to verify an HMAC signature, the recipient performs the same process. They pass the message and the same signing key into the HMAC algorithm, and get back an HMAC signature. If the signature that they calculate matches the signature that they received from the sender then - assuming that the secret signing key has not been exposed - the recipient can be confident that the message has not changed and so has not been tampered with. This shows how HMAC signatures provide authentication, although it doesn't show how they preserve deniability. We'll come back to that in a few sections' time.

[TODO-pic]

In order to use HMAC signatures to authenticate their messages, the sender and recipient need to agree on a shared secret signing key that they can pass into the HMAC algorithm. In OTR they use the *hash* of their shared secret encryption key as their shared secret signing key.

<img src="/images/otr/otr-18.png" />

As mentioned several sections ago, a hash function is a function that produces a random-seeming but consistent output (often called simply a "hash") for each input. Given an input it is very easy to calculate the hash output. By contrast, given a hash output it is essentially impossible to calculate the input that produced it.

There exist many different hash functions, but in OTR Alice and Bob use the hash function TODO to convert their secret encryption key into a secret signing key. TODO is a *cryptographic hash function*, which is a hash function with some specific security properties. Using the hash of their encryption key as their signing key is convenient, since it removes the need for Alice and Bob to perform another key-exchange dance. It also provides a subtle contribution towards deniability that we will discuss later.

### 2.5 [Sending the message](#25-sending-the-message)

Now Alice and Bob are finally ready to securely exchange a message. They have already agreed on a shared encryption key. They calculate the hash of this key to give them their shared signing key. Alice signs her plaintext message using the shared signing secret and the HMAC algorithm. She encrypts both her message and signature using the shared encryption secret and a stream cipher with AES in counter mode. She then sends her encrypted message and signature to Bob.

TODO encrypt then sign?

### 2.6 [Decrypting and verifying the message](#26-decrypting-and-verifying-the-message)

Once Bob receives her message he performs the same process in reverse: he decrypts the message in order to read it, and verifies the HMAC signature in order to ensure it has not been tampered with. Alice and Bob have exchanged a secret, authenticated, forward-ly-secret, and deniable message.

But there's more.

### 2.7 [An unexpected twist: publishing the signing key](#27-an-unexpected-twist-publishing-the-signing-key)

Penultimately and oddly, Alice *publishes* her and Bob's shared signing key to the world, for example by uploading it to a webpage that she controls. This is safe to do, because now that Bob has authenticated Alice's message, it doesn't matter if Eve has access to the signing key that he used to do this. This is the same principle by which it would be safe for Google to publish their private DKIM signing keys when they rotate them.

By contrast, it would be highly unsafe for Alice to publish their shared *encryption* key, because if Eve had stored their encrypted traffic then she would be able to use the encryption key to decrypt it. Happily, since the signing key that Alice publishes is only a cryptographic hash of the encryption key, it cannot be used to reconstruct the encryption key. This shows that publishing the signing key is a safe thing to do, but it doesn't say anything about why it's useful. We'll talk about this shortly.

### 2.8 [Forgetting the encryption key](#28-forgetting-the-encryption-key)

Finally, once Bob has received, decrypted, and verified a message, and Alice has published the private signing key, the participants forget and delete their session encryption key. This step is necessary in order to preserve forward secrecy and to make it impossible for an attacker to ever penetrate their encrypted traffic. Alice and Bob then start the dance again and negotiate a new shared session encryption key for their next message.

Let's summarize these steps:

1. Sender and recipient agree on a secret encryption session key using Diffie-Hellman key exchange. They prove their identities to each other by signing the intermediate values in the exchange
2. Sender signs their message with an HMAC. They use a signing key equal to the cryptographic hash of the encryption key from step 1
3. Sender uses the encryption key from step 1 to encrypt their message and signature
4. Sender sends the encrypted ciphertext to the recipient
5. Recipient decrypts the cipherext and verifies the accompanying signature
6. Sender publishes the HMAC key
7. Sender and recipient forget their session keys

That's how you send an OTR message, but it doesn't say much about why this fiddly foxtrot is useful. How exactly does it preserve deniability? Why does OTR use HMAC signatures? Why does it use a malleable cipher? And why on earth do participants publish their private signing keys once they're done with them?

Let's find out.

----

## 3. The key insights of OTR

### 3.1 Why does OTR use symmetric HMAC signatures instead of an asymmetric algorithm?

OTR goes to significant lengths to use symmetric HMAC signatures to authenticate its messages, instead of asymmetric ones. However, asymmetric signatures generated using public/private keypairs would also do a perfectly good job of authentication. Why does OTR bother with symmetric ones?

The answer is that symmetric signatures help preserve deniability. They help OTR avoid the Podesta problem, in which Wikileaks used (asymmetric) DKIM signatures to prove that the stolen dump of John Podesta's emails was legitimate.

Here's how symmetric signatures help with deniability. Remember, since HMAC signatures are symmetric, they are both created and verified using a single shared secret key that is known to both the signer and the verifier. The signer creates the signature by passing their message and the shared secret key into (in OTR's case) the HMAC signing algorithm. When a verifier needs to verify this signature, they do so by performing the same operation as the signer, using the same key, and comparing their result with the original signature.

[TODO-pic of HMAC verifiying]

Suppose that an attacker completely compromises Alice's computer. They steal a pile of messages that she exchanged with Bob over OTR. The attacker wants to be able to prove to Wikileaks that the messages are real, so they also steal the messages' HMAC signatures. And since in order to verify an HMAC signature you need to know the shared secret, the attacker steals the symmetric signing keys that were used to generate these signatures too.

The attacker goes to Wikileaks and attempts to use the messages' HMAC signatures to prove that their stolen message dump is real. For each message they pass the message's contents and the HMAC secret into the HMAC algorithm. They then show Wikileaks that this signature matches the one in the stolen dump. For an asymmetric signature, this would be strong proof of the messages' legitimacy.

However, for a symmetric signature it doesn't prove anything! Since HMAC signatures are symmetric, the same key is used to both generate and verify them. This means that the attacker could trivially have forged the signatures themselves, and they have no way to prove to Wikileaks that they didn't, even if they did in fact steal them fair and square.

The key difference between the symmetric and asymmetric cases is that asymmetric signatures are generated and verified using different keys. An attacker can therefore verify stolen signatures without having been able to generate them themselves. When dealing with symmetric signatures, the attacker has *too much* power.

## 3.X Why is it necessary and acceptable for OTR to sign its intermediate Diffie-Hellman values using asymmetric signatures?

We've been rattling on about how ingenious and important it is that OTR signs its messages using a symmetric algorithm and a shared secret key. This allows OTR to provide authentication while still preserving deniability. Alice and Bob can be confident that they are talking with each other directly, while also allowing them to deny that they ever spoke in the event that their communication is compromised.

However, the only reason that they are able to trust these symmetric signatures to provide authentication is that they trust that they agreed on their secret signing key with the right person. If an attacker were able to manipulate their key exchange process then they might be able to trick Alice into unwittingly agreeing a shared secret with them instead of Bob. The attacker would then be able to talk to Alice while impersonating Bob.

As we discussed briefly in the Diffie-Hellman key exchange section above, 

After agreeing a shared secret signing key, Alice and Bob use that key as evidence that they are talking with the right person. But how do they know that they agreed this key with the right person in the first place? We can talk about the deniability perils of asymmetric cryptography all we want. But at some point, if you want to be sure that you're talking to the right person on the internet, you're probably going to have to use asymmetric signatures and rely on the other person's public/private keypair.

Nonetheless, OTR provides strong authentication whilst preserving deniability by being very careful about what information it signs and how. You may recall that Alice and Bob agree on their shared symmetric encryption key using a Diffie-Hellman key exchange process. They then take the hash of their encryption key, and use this as their signing key.

A quick recap: in a Diffie-Hellman key exchange, the participants each generate a long random number. They don't send each other their random numbers, but instead exchange carefully selected "intermediate values". Thanks to some spectacular mathematics, by combining their own random number with the other party's intermediate value, both participants can generate the same shared secret key. Just as remarkably, even if an attacker intercepts their traffic and reads both of their intermediate values, the attacker is unable to construct the shared secret key, since they don't know either of the original random numbers.

In order to give each other confidence that they are performing a Diffie-Hellman key exchange with the right person, Alice and Bob each sign their intermediate values using their private keys before sending them. Alice and Bob can verify this signatures, giving them confidence that the intermediate values were indeed generated by the other, and that they are indeed performing a key exchange with the right person. They can then trust that the shared secret derived from the key exchange is known only to the other, and so any messages signed using it and a symmetric algorithm (like HMAC) are valid.

Signing a Diffie-Hellman intermediate value with an asymmetric algorithm and a private key doesn't technically impair participants' deniability. If an attacker intercepts the signed messages sent during key exchange then all they can prove is that Alice and Bob exchanged a few random numbers. This might help as part of a circumstantial case, but at least Alice and Bob's cryptography doesn't mathematically incriminate them.

TODO - also confidence that the other can't rat you out

[TODO-transition? Make it clear why HMAC sigs work here but not for PGP, or say that we are going to talk about it later]

### 3.2 Why does the sender publish the shared HMAC signing key?

We've seen how HMAC signatures are used in OTR for "deniable authentication". But why on earth does the sender go to the effort of publishing the shared signing key to the world once they're done with it? The reason is similar, but subtly different, to the reason that Matthew Green called on Google to publish their DKIM signing secret keys several sections ago.

In OTR, a message signature doesn't need to provide everlasting proof of a message's authenticity to all people for all time. Only the message recipient needs to be confident that an OTR message's signature is valid, and they only need to be confident of this when they are initially receiving the message and checking its signature. Once the recipient has used a signature to authenticate a message, they can record the fact that the message was valid. They need never look at the signature again, and they need never trust it again.

In fact, it's desirable that an OTR siganture provides proof of a message's validity to as few people as possible for as short a window as possible. Once a recipient has used a signature to verify a message's validity, we want to make the signature as useless as possible to anyone who might have stolen it and who might want to use it to prove, or at least provide evidence, that the corresponding message is real. We want to blow the whole system up.

We've seen that OTR signatures are already close-to-useless to attackers because they are generated using the symmetric HMAC algorithm. An attacker can't ever use HMAC signatures to authenticate their swag to a sceptical third-party, because the third-party knows that the attacker could have trivially faked them. The attacker is in this predicament whether or not the participants publish their private signing keys.

Nonetheless, the attacker can still use the HMAC signatures they've stolen to give themselves and their trusted accomplices additional confidence that the corresponding messages are genuine. From the attacker's point of view, the HMAC keys are known only to Alice, Bob, and the attacker. Since the attacker knows that they didn't fake the messages, they can be confident that they were legitimately written and signed by Alice and Bob, even though they can't cryptographically prove this to anyone else. If the attacker has accomplices who trust them implicitly then those accomplices can be similarly confident in the messages' veracity. For example, a court might trust an intelligence agency not to fake HMAC signatures, even though they could, and so take signatures as strong evidence of a message's genuineness.

However, by publishing their ephemeral HMAC signing keys, Alice and Bob make it harder for the attacker to be certain that their haul is genuine. Once anyone can see their ephemeral HMAC signing keys, anyone could have written and signed the messages and left them lying around on Bob's hard drive. Admittedly, the most plausible explanation for how the messages got there is still that Alice and Bob wrote and signed them, but publishing their signing keys is still a cheap and cunning way for Alice and Bob to introduce some extra uncertainty and deniability into the mix. The attacker can't be as certain as they used to be that their stolen messages are real, and anyone that they give the messages to has to trust not only that the attacker is being honest (as before), but also that the messages weren't forged by a fourth-party. It's not an "I am Spartacus" moment so much as a "she is Spartacus, or maybe he is, or perhaps she is, I don't know, leave me alone."

### 3.3 Why does OTR use a malleable encryption cipher?

One of the main goals of OTR is to make it as plausible as possible for the author of an encrypted, authenticated message to claim that they did not write it. We've already seen how OTR signatures are designed to crumble to dust when compromised by an attacker. However, as we'll discuss, it's still possible for even an unsigned ciphertext to make tricky-to-deny ties back to its true author. These ties aren't as mathematically bulletproof as a signature, but in the interests of completeness OTR uses a malleable encryption cipher to try to sever them too.

For many encryption ciphers, it's hard to produce an encrypted ciphertext that decrypts to anything meaningful if you don't know the encryption key. It's not quite hard enough that you can assume that any ciphertext that decrypts to a sensible plaintext must have been generated by someone with access to the secret key, but it is still very hard. This means that if a ciphertext produces sensible plaintext when decrypted then it's reasonable to infer that it was probably generated by someone with access to the encryption key.

[TODO-pic?]

To mitigate this incomplete but still undesirable connection, OTR performs its encryption using a *malleable* stream cipher. A malleable cipher is one that makes it comparatively easy for an attacker to produce a ciphertext that decrypts to something sensible, even if they don't know the encryption key. An attacker can do this by correctly guessing the plaintext that a stream cipher's ciphertext decrypts to. If they can do this, the attacker can manipulate the ciphertext so that it decrypts to any message of their choice of the same length.

<img src="/images/otr/otr-21.png" />

This tweakability means that using a malleable cipher gives Alice and Bob an extra layer of deniability, very similar to the one that they get from publishing their HMAC signing keys. Let's consider a scenario in which this layer might be useful. Suppose that Alice and Bob accidentally use a weak random-number generator (RNG) when choosing their random secrets at the start of their Diffie-Hellman key exchanges. This means that Eve is able to deduce the values of their random secrets. She can use them to work out their symmetric encryption keys, and use these keys to decrypt Alice and Bob's messages. This is already a very bad outcome, but OTR's goal in disasters like this is to mitigate the mishap and make the revealed messages as deniable as possible.

Alice and Bob signed their messages using a symmetric HMAC key, not their private keys. This already gives them a lot of deniable wiggle room. But even though Eve can't cast-iron prove anything, she can still try to build a case on-the-balance-of-probability. 

Alice and Bob signed parts of their Diffie-Hellman key exchange using their private keys. Since they used a weak RNG to generate their secrets, Eve knows their secret keys. She can use the asymmetric signatures on their intermediate Diffie-Hellman values to prove that Alice and Bob performed a key exchange that produced a specific symmetric session key. Eve can then also produce the encrypted messages that Alice and Bob exchanged, and demonstrate that the session key decrypts these messages into sensible plaintexts with valid symmetric signatures.

Even without a malleable cipher, this isn't a total deniability disaster. Symmetrically encrypted ciphertexts are just as useless for proving authorship as symmetrically signed messages. If Eve is able to verify a symmetric signature then she must also have been able to forge it, and if Eve is able to decrypt a symmetrically encrypted ciphertext then she must have been able to forge that ciphertext too. Eve therefore can't use Alice and Bob's ciphertexts to rigorously prove to other people that they wrote them, because Eve could have produced the ciphertexts herself. This is a general property of symmetric encryption, and applies even if the cipher that Alice and Bob use is not malleable.

However, as with HMAC signatures, the fact that the ciphertexts decrypt to a sensible plaintext does give Eve herself a lot of confidence that they are genuine, as well as anyone else who implicitly trusts Eve. If Eve knows that the symmetric encryption key was known only to Alice, Bob, and herself, and she knows that she didn't produce the ciphertext, then she knows that Alice or Bob must have.

To solve a similar problem with their HMAC signatures, Alice and Bob injected some extra ambiguity by publishing their HMAC signing keys after they had been used. This made it clear that anyone could have generated the signatures, not just Alice, Bob, or Eve. Eve could still assume that the signatures were probably generated by Alice or Bob, but now she also has to account for the increased possibility, however slight, that they were forged by a fourth-party.

Similarly, by using a malleable cipher, Alice and Bob make it more plausible that a ciphertext that can be legibly decrypted using their symmetric encryption key could also have been produced by a fourth-party. All that this fourth-party would have had to do is intercept one of their real encrypted messages, correctly guess its plaintext version, and exploit the malleability of the stream cipher used to generate it. This isn't trivial, but it's much easier than if Alice and Bob used a more robust, non-malleable cipher.

[TODO- not note that]

Note that even though stream ciphers are easier to tamper with than many other types of cipher, OTR participants are protected from their messages being blindly tampered with by the bolted-on HMAC signature, so long as the attacker has not compromised their encryption session key. If an attacker who does not know their encryption session key is able to successfully tamper with a message then the HMAC signature will no longer be valid, and the participants will know that they are under attack. If the attacker does know their encryption session key then the game is unavoidably up, and OTR goes into deniability-preserving disaster-defence mode as described above.

### 3.4 Why is the hash of the encryption key used as the signing key?

Alice and Bob use a Diffie-Hellman key exchange to agree on their symmetric encryption key. They could also use a second Diffie-Hellman key exchange to agree on their symmetric signing key, but they don't. Instead, they calculate the hash of their encryption key, and use the result as their signing key.

[TODO-pic]

Linking encryption and signing keys in this way offers an interesting benefit. It means that an attacker who compromises an encryption session key necessarily also compromises the corresponding signing key, whether they like it or not. All they need to do to derive the signing key is to take the hash of the stolen encryption key.

Oddly, this extra exposure benefits Alice and Bob. Suppose that an attacker is able to compromise one of their symmetric encryption keys (perhaps becasue of a weak RNG in their key exchange) and read a message and signature. Because the signing key is just the hash of the encryption key, the attacker also 

 can decrypt their message and signature must necessarily be able to update the signature themselves.

To see why, consider that the worst-case situation - which OTR tries extremely hard to render impossible - is the one in which an attacker:

1. Has access to a decrypted, signed message
2. Can prove to a sceptical third-party that the message's signature is valid
3. Could *not* have produced the signature themselves

This means that an attacker wants to compromise the key 



We've already discussed several other strategies that make it inconveniently easy for an attacker to forge signatures and messages, such as using symmetric ciphers and publishing signing keys to the world. We can still think of some convoluted scenarios that might allow an attacker to strongly suggest to a sympathetic observer that their stolen messages are real.

Suppose that encryption and signing keys weren't linked in any way. An attacker initially steals some messages, their signatures, and their symmetric encryption key, but not the corresponding signing key. The attacker decrypts the messages and shows them to a sceptical third-party, along with the signatures. Since they haven't yet compromised the signing key, they can't verify the signatures. However, without the signing key, they also couldn't have forged the signatures themselves. They are therefore able to satisfy their sceptical third-party that they didn't fabricate the signatures themselves. If the attacker can prove that the signatures match the messages, the third-party will accept them as valid.

The attacker goes back on the offensive. Some time later, they also compromise the messages' symmetric signing key, which they can use to prove to their third-party that the original stolen signatures and messages are valid.

If the attacker had compromised both the encryption key and the signing key at the same time, 

The attacker was able to use the period when they had compromised the encryption key but not the signing key

 Even though the attacker could now use this key to forge signatures, they've already satisfied their sceptical third-party that the signatures are real during the period where they had access to the encryption key but not the signing key. Now that the attacker also has the signing key, they can prove that the signature authenticates the corresponding message.

By linking encryption and signing keys, we make it impossible to have an intermediate period where an attacker knows an encryption key but not a signing key. They could still claim that they've never read [Borisov, Goldberg and Brewer 2004][otr-paper] and so they didn't know that they could use a stolen encryption key to derive the corresponding signing key, but if they've got this far in their attack then that would be hard to believe.






TODO - still safe to publish signing key because doesn't give encyrption

### 3.5 The Principle of Most Privilege

You may have heard of the Principle of Least Privilege. This security precept states:

> Any user, program, or process should have only the bare minimum privileges necessary to perform its function.

Applying Least Privilege to your system means that an attacker who compromises one part of it won't be able to easily pivot into new parts and powers, and will therefore only be able to do limited damage. This is why spies operate in small cells; if they are discovered and interrogated then they don't have enough knowledge or privileges to help their captors roll up their chain of command. This is also why you shouldn't give everyone in your company administrator access to all of your systems. You might trust everyone without reservation, but giving a user unnecessary powers needlessly worsens the consequences of their account getting compromised. It's bad if an attacker is able to read all of one person's emails; it's much worse if they can read everyone's emails, delete them all, and forge new messages that they look like they came from the CEO.

Nonetheless, I would argue that OTR goes out of its way to achieve the opposite: The Principle of Most Privilege! Whereas the goal of the Principle of Least Privilege is to minimize the power gained by an attacker who compromises one part of a system, the goal of the Principle of Most Privilege is to give an attacker who compromises one part of a system total power over all of it.

To see why this is oddly desirable for OTR, remember that the worst-case situation that OTR tries to prevent at all costs is the one in which an attacker:

1. Has access to an unencrypted, signed message
2. Can prove to a sceptical third-party that its signature is valid
3. Could *not* have produced the signature themselves

This combination is a particular disaster because it means that the attacker can read Alice and Bob's messages, *and* prove their legitimacy to a sceptical third-party. It's important that the attacker can prove that a signature is valid (point 2), but *couldn't* have produced the signature themselves (point 3). This proves to the third-party that the attacker didn't forge the signatures themselves. The attacker's capabilities are limited, which is Least Privilege in action but in an oddly unhelpful way.

As we've seen, OTR prevents these 3 properties from occurring simultaneously by making it impossible for points 2 and 3 to be true at the same time. If an attacker can prove that a signature is valid (point 2) then they must have its symmetric HMAC key and so *could* have produced it themselves (the opposite of point 3). This is the Principle of Most Privilege - if you want to verify signatures, then you also have to be able to create them, whether you want this power or not.

The ideal situation is of course that users' communications stay secret and no attacker has any access to anything. Any sensible encryption scheme works well when everything goes as planned, but OTR puts substantial focus on what happens when it doesn't. Usually we want to segment powers and credentials; this is an odd situation in which we don't.

NB: whilst I think it is a useful way to understand OTR, the Principle of Most Privilege is not a real principle, I just made it up. Don't bother Googling it.

----

## 4. CONCLUSION

TODO SOMETHING END


https://en.wikipedia.org/wiki/Off-the-Record_Messaging
Socialist millionaire protocol for key fingerprints




[otr-paper]: https://otr.cypherpunks.ca/otr-wpes.pdf

TODO: lots of layers of deniability that make things even less plausible

TODO: ah ha but you can't mathematically prove that I did it! Yeh but you clearly did, didn't you?

TODO: should probably re-reference PFS in the method